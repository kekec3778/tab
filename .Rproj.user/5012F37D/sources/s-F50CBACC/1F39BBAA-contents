% Style:

% 1. Y|(X,C) has no space between X and C, and wraps X,C in parentheses.
% 2. f(X,Y) never space when writing densities.
% 3. Uppercase or lowercase letters for random variables? Upper except in 
% likelihood expressions?
% 4. f(X|Y) f(Y) put cdot in between. 
% 5. cdots between terms in products.
% 6. Space between parameters in distribution, e.g. X ~ N(0, 1).
% 7. Refer to 2+ variables in text, use (X, Y).
% 8. Use lowercase letters for parameter subscripts, e.g. beta_x, not beta_X. 


\documentclass[fleqn, 10pt]{report}
%\documentclass[fleqn]{apa6e}

\newcommand{\mychapter}[2]{
    \setcounter{chapter}{#1}
    \setcounter{section}{0}
    \chapter*{#2}
    \addcontentsline{toc}{chapter}{#2}
}

\usepackage{hyperref}
%\usepackage[backend=bibtex, sorting=none]{biblatex}
%\bibliography{bibtexfile}

\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage[medium]{titlesec}
\usepackage{bm}
\usepackage{fullpage}
\usepackage{pdflscape}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}
\setlength{\textfloatsep}{20pt}
%\DeclareMathSizes{10}{9}{7}{7}
%\DeclareMathSizes{10}{8}{7}{7}

\usepackage{setspace}
 \doublespacing

\titlespacing*{\section}
{0pt}{5.5ex plus 1ex minus .2ex}{4.3ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{5.5ex plus 1ex minus .2ex}{4.3ex plus .2ex}

\setlength{\mathindent}{35pt}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\begin{document}

<<setup, include = F, echo = F, eval = T>>=
options(replace.assign=TRUE,width=80)
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())

# Load packages
library("xtable")

# Set xtable options
options("xtable.caption.placement" = "top", 
        "xtable.include.rownames" = FALSE, 
        "xtable.sanitize.text.function" = identity,
        "xtable.sanitize.colnames.function" = identity)
@



% \title{Measurement error methods for unmeasured \\ confounding and pooling \\ \vspace{3in} Advisor Bob Lyles}
% 
% \author{Dane Van Domelen \\ 
% \begin{aligned}
% Advisor$:$ Bob Lyles \\
% Committee members$:$ Amita Manatunga \& Eugene Huang
% \end{aligned}
% }
% 
% \maketitle

\begin{titlepage}
    \begin{center}
        \vspace*{3cm}
        
        \Large
        \textbf{Measurement error methods for unmeasured \\ confounding and pooling}
        
        \large 
        
        \vspace{1cm}
        Dissertation by Dane Van Domelen
        \vspace{0.5cm}
        
        July 13, 2016\\
        \vspace{2cm}
        
        \textbf{Advisor:} \\
        Robert H. Lyles\\
        \vspace{0.5cm}
        
        \textbf{Committee members:} \\ 
        Amita K. Manatunga, Yijian Huang, \& Emily M. Mitchell
        
        \vspace{1.5cm}
        
        \vspace{2cm}
        
        
        Department of Biostatistics and Bioinformatics\\
        Rollins School of Public Health\\
        Emory University\\
        
    \end{center}
\end{titlepage}

\tableofcontents

% \mychapter{0}{Scope}
% %\chapter*{Scope}
% 
% 
% 
% The focus of this chapter is the scenario where one would like to fit a multivariable regression model, but one or more of the covariates are not available. This is a common occurrence in secondary data analysis, where the study was not designed to answer the research question of interest, and in the analysis of data from disease registries, which often contain only basic demographic information. Fitting the regression model of interest without the missing covariates generally results in biased estimates for the adjusted associations of interest.
% 
% For such a scenario to truly be termed unmeasured confounding, the missing covariate must be a confounder. That implies that one of the measured variables is the exposure of interest, the goal is to estimate its causal effect on the outcome variable, and a collection of confounders to control for has been identified.
% 
% That is certainly possible, but our focus is more broad. The corrective methods we consider do not typically require a causal inference framework. They apply to the scenario where an analyst would like to fit a regression model and interpret the adjusted associations, but one or more covariates are not available.
% 
% In some cases, researchers concerned about unmeasured confounding may have access to a different dataset that includes the unmeasured confounder(s) and the other covariates of interest. When such validation data is available, one can choose from a number of corrective methods, such as propensity score calibration, maximum likelihood or regression calibration approaches from the measurement error literature, or missing data techniques like multiple imputation. 
% 
% The purpose of this chapter is to examine the validity and efficiency of methods available to address unmeasured confounding, and to provide recommendations for epidemiological research.


\mychapter{0}{Abstract}

Epidemiologists increasingly utilize existing datasets to explore exposure-disease relationships. A common problem in secondary data analysis is that one or more potentially important confounders may not be available. In Chapter 1, we compare methods for handling unmeasured confounding when validation data is available or can be collected. We consider propensity score calibration as well as maximum likelihood and regression calibration approaches from the measurement error literature, both of which require specifying a model for the unmeasured confounder given the exposure, disease model covariates, and any additional informative covariates. We present simulation results for various linear and logistic regression scenarios where validation data are either internal (include outcome data) or external. We apply the three methods to assess whether low Vitamin D is associated with lower odds of successfully becoming pregnant, controlling for age, obesity status, and caloric intake, using data from a clinical trial designed for a different purpose.

Pooling refers to the scenario where biomarkers are assayed in samples containing specimens from two or more individuals. In many common scenarios, a pooling study design can drastically reduce costs while achieving the same power as an individual-level design, or drastically improve power for a fixed budget. However, two types of error threaten the validity of poolwise analysis: measurement error from an imperfect assay, and processing error from combining specimens into pools. In Chapter 2, we present maximum likelihood methods for linear and logistic regression where a pooled exposure is subject to both types of error. We assume the exposure is normally distributed (conditional on covariates if there are any), and measurement and processing errors are independent, normally distributed, and not dependent on pool size. Under these assumptions, a hybrid pooling design with at least three different pool sizes permits valid estimation of regression coefficients. If one of the pool sizes is 1, all variance components are estimable. Simulation studies suggest that the ML estimator accounting for both error types is right-skewed and unstable even for fairly large studies (e.g. 1,200 total pools). Adding a small number of individual replicate measurements drastically improved the behavior of the ML estimator, while also permitting identifiability without any pool size requirements.

The measurement and processing error correction in Chapter 2 required normality of the error-prone exposure conditional on covariates. For right-skewed biomarkers, a log-normal distribution might be more reasonable. With individual measurements, this modification would be straightforward. However, a pooled measurement gives the sum of the individual biomarker levels for all members of a pool, while the sum of several log-transformed values does not equal the log-transformed sum of the values. In Chapter 3, we plan to extend methods developed for analyzing poolwise data on right-skewed outcomes to the scenario where an exposure is right-skewed and subject to measurement and processing errors.


\mychapter{1}{Chapter 1: Measurement error methods in the unmeasured confounding setting}

\section{Motivation} \label{Motivation}

We consider the situation in which an investigator wishes to fit a generalized linear model to estimate the association between an exposure $X$ and an outcome $Y$ adjusted for covariates $(\bm{C}, \ \bm{Z})$:
%
\begin{equation}
\label{tdm1}
g[E(Y)] = \beta_0 + \beta_X X + \bm{\beta_C}^T \bm{C} + \bm{\beta_Z}^T \bm{Z}
\end{equation}
%
But the dataset of interest is missing $\bm{Z}$. This is a very common problem in observational research, as epidemiologists often utilize existing data from large population-based studies or disease registries to study exposure-disease relationships \citep{smith2011conducting}. Perhaps more often than not, the most relevant existing dataset to explore a particular research question does not include data on every single variable of interest. 

In such a scenario, the simplest approach is to fit model 1.1 without $\bm{Z}$, and obtain what might be termed the ``naive'' estimates $\hat{\bm{\beta}}^* = (\hat{\beta}_X^*, \ \bm{\hat{\beta}_C}^*)^T$. These estimates are generally biased for $\bm{\beta} = (\beta_X, \ \bm{\beta_C})^T$, but may be informative in cases where the direction of confounding due to $\bm{Z}$ can be determined. For example, as stated by \citet{vanderweele2008causal}, if $\hat{\beta}_X^*$ is statistically and clinically significant, and identified as a lower bound for $\beta_X$, then the effect of $X$ on $Y$ controlling for $\bm{C}$ and $\bm{Z}$ is also statistically and clinically significant.

Another approach is to use sensitivity analysis along the lines of \citet{vanderweele2011unmeasured} and \citet{lin1998assessing}. These methods allow one to determine how strong the unmeasured confounding effect would have to be to explain away the observed effect estimate. One may be left with an unsatisfying conclusion if that degree of confounding is plausible, or if negative confounding is suspected and the observed exposure effect is small or not statistically significant.

In many cases, one would prefer an unbiased and consistent method for directly estimating $\bm{\beta}$. When validation data on $(X, \ \bm{C}, \ \bm{Z})$ are available or relatively inexpensive to obtain, corrective methods from the measurement error and missing data literature come to mind. 

One could employ variants of regression calibration (RC) used, for example, by \citet{weller2007regression}, \citet{kipnis2012regression}, and \citet{lyles2013approximate} for handling covariate measurement error. These methods require specifying a model for the expected value of $\bm{Z}$ given $(X, \ \bm{C})$ and perhaps additional covariates $\bm{D}$ available in both the main study and validation study. In the case where $Z$ is scalar and continuous, validation data may support the linear model:
%
\begin{equation}
\label{mem1}
E(Z) = \alpha_0 + \alpha_X X + \bm{\alpha_C}^T \bm{C} + \bm{\alpha_D}^T \bm{D}
\end{equation}
%
In the \citet{lyles2013approximate} application of RC, ordinary least squares (OLS) is used to obtain $\hat{\bm{\alpha}}$, and model \ref{tdm1} is fit with $E(Z|X, \bm{C}, \bm{D}; \ \hat{\bm{\alpha}})$ in place of the unobserved $Z$'s. Regression calibration gives consistent estimates of $\bm{\beta}$ in linear regression \citep{carroll2006measurement}; approximately consistent estimates in logistic regression under certain conditions \citep{rosner1989correction, kuha1994corrections}; and often performs well in other generalized linear models \citep{carroll2006measurement}. 

With validation data on hand, one might be comfortable fully specifying the distribution of $\bm{Z}|(X, \bm{C}, \bm{D})$, and performing a maximum likelihood (ML) analyis \citep{lyles2013approximate}. For example, one could modify the linear model \ref{mem1} to include random errors $\delta$ that are assumed to be normally distributed. Compared to regression calibration, a two-model ML approach has some advantages (efficiency, flexibility) and some disadvantages (less ease of implementation, less numerical stability).

If primary interest is in $\beta_X$, and $X$ is a binary exposure, yet another option is to use propensity score calibration, a method developed by \citet{sturmer2005adjusting} specifically to handle unmeasured confounding. Briefly, this would involve obtaining validation data and fitting a model for the probability of $X$ given the main study covariates $\bm{C}$; a model for the probability of $X$ given the full covariate vector $(\bm{C}, \ \bm{Z})$; and a linear model relating the two. In the main study, $\hat{P}(X|\bm{C})$ is calculated for each observation and then mapped to $\hat{P}(X|\bm{C}, \bm{Z})$. Finally, model \ref{tdm1} is fit with $X$ and $\hat{P}(X|\bm{C}, \bm{Z})$ as predictors. This method is computationally simple and generalizes nicely to the case of several unmeasured confounders, but its validity depends critically on a surrogacy assumption that may be difficult to assess (see Section \ref{Propensity score calibration}, page \pageref{Propensity score calibration}).

To our knowledge, measurement error methods like regression calibration and maximum likelihood have not typically been applied to the unmeasured confounding setting. This application seems natural. However, there are some aspects of the unmeasured confounding scenario that distinguish it from the measurement error scenario. For example, error-prone versions of true predictors are never observed; in effect, there is no ``measurement error.'' As a result, validation data is useful, while replication data is not. Perhaps more importantly, valid estimation via ML or RC often requires identifying one or more variables that inform $\bm{Z}$ but not $Y$ given $(X, \ \bm{C}, \ \bm{Z})$. Finally, in evaluating validity and efficiency, the regression coefficient for the error-prone variable is of primary interest in the measurement error setting, while a regression coefficient for a perfectly measured variable ($X$) is generally of primary interest in the unmeasured confounding setting.

This chapter is organized as follows. First, background material on confounding, directed acyclic graphs, and measurement error concepts is provided. Second, methods of interest for handling unmeasured confounding are discussed in detail. Third, a number of specific unmeasured confounding scenarios are considered, and simulation studies are provided to assess validity and efficiency. %Modifications to existing methods are described where appropriate. 
Fourth, we apply the various methods to a reproductive health study to estimate the effect of Vitamin D intake on odds of successfully becoming pregnant, controlling for covariates including total caloric intake, which is not measured in the main study. Finally, we summarize our findings and provide recommendations for epidemiological research.





\newpage

\section{Background}
% %\chapter*{Background}
% 
% 
% 
% \section{Multivariable regression modeling}
% 
% 
% 
% There are numerous reasons a researcher may wish to fit a multivariable model for a health outcome of interest. The goal may be to build a predictive model, for example to estimate disease risk based on a patient's presenting characteristics. A famous example is the Framingham Risk Score, which estimates 10-year risk of cardiovascular disease (CVD) based on sex, age, total cholesterol, high-density lipoprotein cholesterol (HDL), systolic blood pressure (SBP), smoking status, diabetes status, and antihypertensive medication use \citep{d2008general}.
% 
% Alternatively, the goal may be to determine whether a particular variable is associated with a health outcome independent of another variable. For example, in the physical activity literature, there is great interest in determining whether prolonged sedentariness has health consequences beyond those expected due to lack of moderate-to-vigorous intensity physical activity \citep{koster2012association}.
% 
% In epidemiology, perhaps the most common application of multivariable modeling is to estimate the causal effect of an exposure on a disease. In experimental studies, one can simply compare the disease incidence in the exposed group to the disease incidence in the non-exposed group. In observational studies, such a comparison may be insufficient, as differences in disease incidence may be driven by characteristics that differ in the exposed and non-exposed groups rather than the exposure itself. This is termed confounding bias.
% 
% In the first scenario, building a predictive model, one would typically include any and all characteristics that improve estimation of disease risk, whether causative in nature or merely prognostic. In the Framingham Risk Score, antihypertensive medication use is prognostic of CVD, but of course does not cause CVD.
% 
% In the second and third scenarios, determining an appropriate set of covariates requires careful consideration. If the goal is to estimate the effect of $X$ on $Y$ that is not through $Z$, as in the sedentary/physical activity example, then a regression model with $Y$ vs. $(X, Z)$, and other covariates $\bm{C}$ may yield the desired estimate. If the goal is to estimate the total causal effect of $X$ on $Y$, then the choice of whether to include $Z$ depends on the presumed directional relationships between the three variables.

\subsection{Confounding}

For the most part, our focus is not on assessing causality or choosing an appropriate set of control variables to obtain an unconfounded effect estimate. We assume that investigators have already specified a regression model of interest, and the regression parameters represent quantities of epidemiologic interest. Still, some background information on directed acyclic graphs, causality, and confounding is warranted. 

\subsubsection{Definition and consequences}


\citet{greenland1999causal} define confounding as the scenario in which the probability distribution for an outcome variable differs across levels of an exposure for reasons other than the effects of the exposure. The variables responsible are termed confounders.

Failure to adequately control for confounding can result in various incorrect conclusions, such as concluding that $X$ affects $Y$ when it truly does not, that $X$ does not affect $Y$ when it truly does, finding a protective or harmful effect when the opposite is true, or over or underestimating the true causal effect.

\subsubsection{Directed acyclic graphs}

Directed acyclic graphs or DAGs are a useful graphical tool for conceptualizing epidemiological relationships and identifying confounders \citep{greenland1999causal}. Three simple graphs are shown in Figure \ref{fig_1_1} to illustrate important concepts. 

\begin{figure}[h]
\includegraphics{fig_1_1a.pdf}
\includegraphics{fig_1_1b.pdf}
\includegraphics{fig_1_1c.pdf}
\centering
\caption{Three graphs showing assumed cause and effect relationships among variables.}
\label{fig_1_1}
\end{figure}

The variables $X$, $Z$, and $Y$ are termed nodes, and the lines connecting them are termed arcs or edges. Single-headed arrows reflect the direction of assumed cause and effect relationships. For example, the first graph reflects the hypothesis that $X$ has a causal effect on $Y$, and $Z$ has a causal effect on both $X$ and $Y$. Lack of a single-headed arrow from one variable to another implies the absence of a direct effect.

A path is any unbroken route that connects nodes. A directed or causal path is one in which each variable on the path causes the next. For example, in the first graph in Figure \ref{fig_1_1}, the $Z-X-Y$ path is a causal path, while the $X-Z-Y$ path is not. The latter path is termed a backdoor path form $X$ to $Y$ because it has an arrow pointing towards $X$. 

%A bidirectional arcs connecting two variables indicates that they share one or more common ancestors that are not shown on the graph. A dashed nondirectional arc signifies a relationship of unspecified causality.

The ``directed'' part of the term directed acyclic graph refers to the fact that all edges have a single or double headed arrow. The ``acyclic'' part refers to the fact that no directed path forms a closed loop. The third graph is not a DAG because it has a cyclic path.

In the first graph, $X$ has a direct effect on $Y$, or in other words $X$ is a parent of $Y$ and $Y$ is a child of $X$. $Z$ has both a direct effect on $Y$ and an indirect effect through the $Z-X-Y$ path.

A variable is considered an ancestor of another if a directed path of arrows connects the first to the second. For example, in the first graph, $Z$ is an ancestor of both $X$ and $Y$ and $X$ is an ancestor of $Y$. Similarly, $X$ and $Y$ are descendents of $Z$, and $Y$ is also a descendent of $X$.

In the second graph, the $X-Y-Z$ path is said to be blocked by $Y$, or the path collides at $Y$, or $Y$ is a collider on the path. The $X-Z-Y$ path is unblocked because it does not have a collider.

Two variables are marginally unassociated if there are no unblocked paths between the two variables. If the $Z-X$ edge in the first graph was not there, then $X$ and $Z$ would be marginally unassociated, because the only path connecting them collides at $Y$.

An unblocked path must be either a directed path or a backdoor path through a shared ancestor. The former suggests a causal effect, while the latter suggests a confounded effect. A combination of the two can occur. For example, in the first graph, the unblocked $X-Y$ path indicates a causal effect of $X$ on $Y$, while the unblocked $X-Z-Y$ path indicates a confounded association of $X$ with $Y$. In other words, the association between $X$ and $Y$ is partially causal and partially confounded. A crude measure of association between $X$ and $Y$ represents the net result of both effects, which may be in the same or opposite directions, and may cancel each other entirely.

A causal effect of one variable on another is said to be mediated by a third variable if the causal path passes through that variable. For example, in the second DAG, $X$ has a direct effect on $Y$ (the $X-Y$ path), but also has an indirect effect on $Y$ through the mediator $Z$ (the $X-Z-Y$ path).

\subsubsection{DAGs: Assessing confounding}

A common goal of epidemiologic analysis is to estimate the total causal effect of an exposure on an outcome. The total causal effect may include direct and indirect effects, e.g. the $X-Y$ direct effect and $X-Z-Y$ indirect effect in the second graph in Figure \ref{fig_1_1}. Once a DAG is constructed, one can determine whether a crude estimate of the exposure effect is potentially confounded by one or several other variables. 

After deleting all single-headed arrows coming from the exposure variable of interest, if there are no unblocked paths from the exposure to the outcome variable, then there is no potential confounding; otherwise, there is. This makes intuitive sense because an unblocked path from exposure to outcome after removing exposure effects means that the variables remain associated for reasons other than the exposure's effect.

In the first graph, the crude $X-Y$ association is confounded, because when you remove the arrow emanating from $X$ the unblocked backdoor path $X-Z-Y$ still connects $X$ to $Y$. In the second graph, the crude $X-Y$ association is not confounded, because when you remove the arrows emanating from $X$ there is no path that connects $X$ to $Y$. The third graph is not a DAG.

\subsubsection{DAGs: Sufficient control for confounding}

Based on DAGs, \citet{greenland1999causal} provide criteria to determine whether controlling for a group of variables $S$ is sufficient to remove confounding. The variables in $S$ must not be descendents of the exposure or outcome. The two criteria are as follows:

\begin{itemize}
  \item[] (1) Any unblocked backdoor path from exposure to outcome is intercepted by a variable in $S$.
  \item[] (2) Any unblocked path from exposure to outcome induced by adjustment for $S$ is intercepted by a variable in $S$.
\end{itemize}

%An equivalent version of (2) is as follows: If every collider on a backdoor path from exposure to outcome is in $S$ or has a descendant in $S$, then $S$ must also contain a noncollider along the path.

\noindent \citet{greenland1999causal} also present a ``backdoor test for sufficiency'':

\begin{itemize}
  \item[] (1) Remove arrows emanating from exposure.
  \item[] (2) Draw undirected arcs connecting pairs of variables sharing a child that is in $S$ or has a descendant in $S$. 
  \item[] (3) Check for unblocked paths from exposure to outcome that do not pass through $S$.
\end{itemize}

\noindent To illustrate, consider the DAG in Figure \ref{fig_1_2} and the goal of estimating the total causal effect of $X$ on $Y$. Consider control for $S = (Z_1, \ Z_3)$.

\begin{figure}[h]
\includegraphics{fig_1_2.pdf}
\centering
\caption{DAG to illustrate sufficient control for confounding.}
\label{fig_1_2}
\end{figure}

Neither $Z_1$ nor $Z_3$ are descendents of $X$ or $Y$. Following (1), we remove the arrow from $X$ to $Y$; (2) requires no action; for (3), we see there are two unblocked paths from $X$ to $Y$: $X-Z_2-Z_1-Y$, which is intercepted by $Z_1$, and $X-Z_2-Z_3-Y$, which is intercepted by $Z_3$. Thus $S = (Z_1, \ Z_3)$ is sufficient to control for confounding. 

However, consider the smaller set $S^* = Z_2$. If we (1) remove the $X-Y$ arrow, and (2) connect $Z_1$ and $Z_3$ since they share the child $Z_2$, we obtain Figure \ref{fig_1_3}.

\begin{figure}[h]
\includegraphics{fig_1_3.pdf}
\centering
\caption{DAG in Figure 1.2 after applying steps (1) and (2) of backdoor test for sufficiency for $S^* = Z_2$.}
\label{fig_1_3}
\end{figure}

There are four unblocked paths from $X$ to $Y$: $X-Z_2-Z_1-Y$, $X-Z_2-Z_3-Y$, $X-Z_2-Z_1-Z_3-Y$, and $X-Z_2-Z_3-Z_1-Y$. All are intercepted by $S^* = Z_2$. So $Z_2$ is also sufficient to control for confounding.

%One would typically prefer to control for confounding with the smallest set of variables possible. A set of control variables $S$ is considered minimally sufficient for adjustment if $S$ is sufficient but no proper subset of $S$ is sufficient. There may be multiple minimally sufficient sets for a particular DAG, and in some cases two or more entirely unique sets of variables $S_1$ and $S_2$ may be minimally sufficient.




\subsubsection{Unnecessary adjustment and overadjustment}

\citet{schisterman2009overadjustment} make the important point that controlling for variables that are not confounders can introduce rather than remove bias and can affect statistical efficiency. 

Unnecessary adjustment refers to the scenario where one controls for a variable that is not a confounder without introducing bias in the estimated total causal effect of the exposure. This may occur when the variable controlled has no causal relationship with exposure or outcome, has a causal effect on the exposure or outcome but not both, or is caused by the exposure or outcome but not both. In linear models, unnecessary adjustment actually increases statistical efficiency when the extra control variable has a causal effect on the outcome variable. In non-linear models, unnecessary adjustment reduces precision even when the extra variable has a causal effect on the outcome \citep{robinson1991some}.

Overadjustment bias occurs when one controls for an intermediate variable on the causal pathway between exposure and outcome. In general, this introduces downward bias in the estimated total causal effect of the exposure on the outcome. Under additional assumptions of no unmeasured confounding for the exposure and outcome and for the mediator and outcome, the quantity estimated represents the direct effect of the exposure on the outcome (i.e. not through the mediator), which may be of epidemiologic interest \citep{cole2002fallibility}.


\subsubsection{Analytic control for confounding variables}

Once a sufficient set of control variables is identified, either through the graphical procedure described by \citet{greenland1999causal} or by some other means, one must perform an analysis that removes the confounding bias. Strategies to control for confounding variables in the analysis stage (as opposed to the design stage) include modeling, stratification, and matching.

In the modeling approach, one fits a multivariable regression model with the exposure and confounding variables as predictors. The regression coefficient for the exposure is interpreted as the effect of the exposure on the outcome holding all other covariates constant. This approach may fail to remove confounding if the relationships between the confounders and the outcome are not modeled correctly (e.g. linear relationship assumed but quadratic relationship exists).

The stratification approach involves estimating the exposure effect across strata of the confounding variables, and computing an aggregate estimate assuming it is similar across strata (no effect modification) \citep{birch1964detection}. This approach is natural when there is a single categorical confounder, or a single continuous confounder converted to quantiles. The number of strata increase with the number of confounders, which can cause sparsity problems.

A third approach is to match exposed subjects to non-exposed subjects with similar values for the confounding variables. This generally results in only a fraction of the observations being used to estimate the exposure effect. Propensity score matching is an appealing extension, as it provides a single measure to match on from a potentially large set of confounders \citep{rosenbaum1983central}. 


\subsubsection{Residual vs. unmeasured confounding}

Residual confounding refers to the scenario in which a confounder is included in an analysis, but its confounding effect is not completely removed \citep{becher1992concept}. For example, if one attempts to control for a continuous confounding variable by estimating the exposure effect in participants above and below the median value for the confounder, confounding bias may remain due to variation in the confounder that still exists in the two strata. In general, analyses are less prone to residual confounding when continuous confounders are treated as continuous or divided into at least five categories \citep{cochran1968effectiveness}.

Unmeasured confounding refers to the scenario in which one or more confounders are not controlled for at all, usually because they are not available \citep{vanderweele2008causal}. 

% In many cases, one might conceptualize an epidemiological relationship, construct a DAG, and identify a dataset to explore it, only to find that one or several confounders are not available. This is especially likely to occur in secondary data analysis, as the study was designed to answer a different research question.
% 
% Failure to adjust for a confounder generally results in a biased estimate of the causal effect of the exposure. Fortunately, numerous corrective methods can restore validity when validation data are available.

%To truly be considered unmeasured confounding, the omitted variable should meet the definition of a confounder. But assessing whether the omitted variable is a confounder requires specifying a particular covariate as the exposure of interest. In many cases, an analyst may not be focused on a particular exposure and whether or not it affects the outcome, but simply wish to fit a multivariable model with a particular set of covariates and interpret the adjusted associations.

%Most of the corrective methods that we consider here do not require that the missing variable(s) meet the criteria to be considered confounders. They apply generally to the problem where one would like to fit a multivariable regression model, but one or more covariates are not available. 

%Here we consider several methods developed for missing and mismeasured data, as well as a method developed specifically for the unmeasured confounding, propensity score calibration.



\subsection{Measurement error concepts}

\subsubsection{The measurement error problem}

Measurement error refers to the scenario in which one or more predictors are measured with error. It is well known that using imprecisely measured predictors in regression modeling usually results in biased parameter estimates and loss of statistical power \citep{carroll2006measurement}. 

In rare scenarios, the effects of measurement error are predictable. For example, if one fits a simple linear regression model where the measured predictor is really the true predictor plus some random error, there will be bias towards the null, and the amount of bias is simply the ratio of variances for the true predictor to the measured variable. This can be useful, as a data analyst aware of measurement error in an exposure might observe a significant effect, and be able to suppose that the true effect is larger in magnitude.

In even rarer cases, measurement error may not induce any bias at all. In the simple linear regression scenario, if the true predictor is really the measured predictor plus some random error (``Berkson'' error; see p. \pageref{Classical vs. Berkson}), then using the measured predictor does gives valid estimates, but with less power than if the true predictor was measured directly. It may seem unusual for the true predictor to be a noisy version of the measured predictor. To illustrate, consider a study in which the experiments attempt to control the exposure, like the humidity level in a room. If the humidity control system is imperfect, it may be reasonable to assume that the true humidity in the room is equal to the target humidity plus some random error.

In most practical scenarios, there is bias introduced by covariate measurement error, and the direction and magnitude of bias depends on the type of measurement error and the relationships among the covariates.

% \subsection{Similarities with unmeasured confounding}
% 
% Many of the methods available to correct for covariate measurement error rely on validation data, or data relating the true variable to the imprecisely measured version that is available. Corrective methods like maximum likelihood and regression calibration utilize the fact that the imprecise version of the predictor informs the true predictor.
% 
% It is natural to consider the same methods for the unmeasured confounding setting, where we utilize the fact that the available covariates inform the missing confounder. The idea follows \citet{weller2007regression}, which presents regression calibration for logistic regression when there are multiple surrogates for a single exposure, and \citet{lyles2013approximate}, which compared various approaches based on a model relating the missing exposure to covariates.
% 
% There has to be some collinearity between the missing predictor and the other disease model covariates, otherwise adjustment for the missing predictor would be unnecessary (and it would not be a confounder). A relatively low degree of collinearity would be analogous to ``large measurement error'' and would result in a considerable loss of efficiency compared to the complete data approach, and perhaps some bias if regression calibration is used and the TDM is something other than linear regression.

\subsubsection{Measurement error terminology}

A thorough coverage of measurement error terminology and definitions is given by \citet{carroll2006measurement}. Here we briefly describe some classifications of measurement error and measurement error methods. For notation, $X$ represents the true value of the error-prone exposure, $W$ represents an imprecise version of $X$, and $\bm{C}$ represents any perfectly measured covariates.

\subsubsection{Functional vs. structural}

While there is no widely agreed upon definition of functional vs. structural measurement error methods, \citet{carroll2006measurement} define structural methods as those in which $X$ is viewed as random, and distributional assumptions on $X$ are imposed. Functional methods are those in which $X$ is viewed either as fixed or as random but with no distributional assumptions imposed.

Maximum likelihood is a structural method, as it requires fully specifying the distribution of $X$, or of $X$ given one or more other variables. Regression calibration, simulation extrapolation, and score function methods are structural methods. Structural methods tend to be more efficient when distributional assumptions are correct, whereas functional methods have a robustness advantage.

\subsubsection{Classical vs. Berkson} \label{Classical vs. Berkson}

Measurement error is considered classical if $W$ is $X$ plus some random error, and Berkson if $X$ is $W$ plus some random error. 

\begin{equation}
\begin{split}
\text{Classical measurement error:} \quad W = X + U, \quad U \sim (0, \ \sigma_u^2)\\
\text{Berkson measurement error:} \quad X = W + U, \quad U \sim (0, \ \sigma_u^2)
\end{split}
\end{equation}
\vspace{4pt}

Under classical measurement error, $\text{V}(W) > \text{V}(X)$, while under Berkson measurement error, $\text{V}(X) > \text{V}(W)$.

In most cases, $W$ is a noisy version of $X$ and it is more natural to assume classical measurement error. But Berkson error is common when experimenters attempt to control the level of exposure applied to study participants, as the true value $X$ ends up being the target level $W$ plus some random error.

\subsubsection{Classical error models vs. regression calibration models}

A similar classification differentiates classical error models, in which the conditional distribution of $W$ given $X$ (and perhaps other covariates) is modeled, from regression calibration models, in which the conditional distribution of $X$ given $W$ (and perhaps other covariates) is modeled. The advantage of regression calibration modeling is that it requires specifying only two models: $Y|(X, \bm{C})$ and $X|(W, \bm{C})$. Classical error modeling typically requires three: $Y|(X, \bm{C})$; $W|(X, \bm{C})$, where $\bm{C}$ is typically omitted; and $X|\bm{C}$.

\subsubsection{TDM and MEM}

\citet{clayton1992models} provide a useful nomenclature for models involved in an analysis that involves measurement error. In the absence of covariates, the true disease model (TDM) relates the outcome $Y$ to $X$; the measurement error model (MEM) relates $W$ to $X$; and the predictor distribution model is the probability density function for $X$. Specifying the predictor distribution model is only necessary when classical error modeling is used. $\bm{C}$ could also be included as a covariate at each level.

\subsubsection{Differential vs. non-differential}

Measurement error is non-differential, or $W$ is termed a surrogate for $X$, when $W$ does not inform $Y$ given $(X, \ \bm{C})$, or $f(y|x, \bm{c}, w) = f(y|x, \bm{c})$ \citep{thomas1993exposure}. When $f(y|x, \bm{c}, w) \neq f(y|x, \bm{c})$, measurement error is said to be non-differential. Equivalently, measurement error is non-differential if $f(w|x, \bm{c}, y) = f(w|x, \bm{c})$, and differential otherwise. 

\subsubsection{Internal vs. external validation data}

In main study/validation study designs, main study participants have data on $(Y, \ W, \ \bm{C})$ but not the true exposure $X$. Validation data are internal if $X$ is measured on a sample of main study participants. In this case $(Y, \ X, \ \bm{C}, \ W)$ are measured in the validation study, and one can model $f(y|x, \bm{c}, w)$ rather than assume $f(y|x, \bm{c}, w) = f(y|\bm{c}, x)$.

Validation data are external if a different dataset is used to characterize the relationship between $W$ and $X$. Data on the outcome variable is typically not available, so only $(X, \ \bm{C}, \ W)$ are measured in the validation study. One has to assume that $f(y|x, \bm{c}, w) = f(y|\bm{c}, x)$ because there is no data to model $f(y|x, \bm{c}, w)$.

Using external validation data requires a transportability assumption \citep{carroll2006measurement}. This means that the nature of the measurement error in the validation study population is the same as in the main study population. This assumption is typically unverifiable and may be suspect when demographics are quite different in the two studies.


\subsection{Measurement error methods - Maximum likelihood}

\subsubsection{Complete data likelihood}

%Maximum likelihood (ML) estimation requires specifying a likelihood function, or the probability distribution for the observed random variables, and finding the parameter values that maximize the likelihood. ML estimation is efficient when the likelihod is correctly specified, but prone to producing invalid estimates when it is not.

%In the measurement error scenario, construction of the likelihood function depends on the nature of the validation data and on whether one uses classical error modeling or regression calibration modeling.

Suppose we wish to fit a regression model for an outcome $Y$ and an exposure $X$, controlling for covariates $\bm{C}$. In the absence of measurement error, only $f(y|x, \bm{c}; \ \bm{\beta})$ has to be specified. Even if $X$ is viewed as random rather than fixed, specifying $f(x, \bm{c})$ is not necessary because it does not inform $\bm{\beta}$.

\subsubsection{Main study likelihood}

Instead of observing $(Y, \ X, \ \bm{C})$, we observe $(Y, \ W, \ \bm{C})$, where $W$ is an imperfect version of $X$. The likelihood contribution for each observation is $f(y, w, \bm{c})$. We use the fact that $f(y, w, \bm{c}) = \int_x f(y, x, w, \bm{c}) \ dx$, and factor the joint density $f(y, w, \bm{c}) = \int_x f(y|x, w, \bm{c}) \cdot f(x, w, \bm{c}) \ dx$. If non-differential measurement error is assumed we replace $f(y|x, w, \bm{c})$ with $f(y|x, \bm{c})$. 

There are several options for specifying $f(x, w, \bm{c})$. Classical measurement error would support the factorization $f(w|x, \bm{c}) \cdot f(x|\bm{c}) \cdot f(\bm{c}) = f(w|x) \cdot f(x|\bm{c}) \cdot f(\bm{c}) \propto f(w|x) \cdot f(x|\bm{c})$. If $W$ is $X$ with additive normal errors, and $X|\bm{C}$ is a linear model with normal errors, then $X|W, \bm{C}$ is normal and one can use $f(x|w, \bm{c})$ rather than $f(w|x) \cdot f(x|\bm{c})$. 

In some cases, one might directly specify a model for $f(x|w, \bm{c})$. The advantage of this approach is that it requires specifying one model rather than two, and validation data can be used to ensure reasonable model fit. The disadvantage is that it avoids describing the precise nature of the measurement error. 

\subsubsection{Likelihood for validation data}

ML estimation of $\bm{\beta}$ typically requires validation data on $(X, \ W, \ \bm{C})$ or replicates, i.e. $W$ measured multiple times on the same participants. Replicates are not useful in the unmeasured confounding scenario, so only validation data are discussed here.

If external validation data are available, $(X, \ W, \ \bm{C})$ are observed and the likelihood contribution is $f(x, w, \bm{c}) \propto f(x, w|\bm{c})$ which can be factored in several ways as described in the previous section. If internal validation data are available, $(Y, \ X, \ W, \ \bm{C})$ are observed and the likelihood contribution is $f(y, x, w, \bm{c}) \propto f(y, x, w|\bm{c}) = f(y|x, w, \bm{c}) \cdot f(x, w|\bm{c})$.

%If there are replicates with $(Y, \bm{W}, \bm{C})$ observed, where $\bm{W} = W_1, ..., W_k$, the likelihood contribution is $f(y, \bm{w}, \bm{c}) = \int_x f(y|x,\bm{c}) \cdot f(x|\bm{w}, \bm{c}) \cdot f(\bm{w}|\bm{c}) \ dx$.

\subsubsection{Likelihood function}

The likelihood function for $n_M$ main observations, $n_E$ external validation study observations, and $n_I$ internal validation study observations is as follows:
%
\begin{equation}
\label{ml_3types}
%\begin{split}
L = \prod_{i=1}^{n_M} \int_x f(y_i|x_i, w_i, \bm{c}_i) \cdot f(x_i, w_i|\bm{c}_i) \ dx \cdot \prod_{j=1}^{n_E} f(x_j,w_j|\bm{c}_j) \cdot \prod_{k=1}^{n_I} f(y_k|x_k, w_k, \bm{c}_k) \cdot f(x_k,w_k|\bm{c}_k)
%\end{split}
\end{equation}
%
Typically one has access to either internal or external validation data, not both, but ML estimation can support any combination of the three data types.

\subsubsection{Maximizing the likelihood}

Optimization routines in statistical software programs can be used to maximize the likelihood function and obtain estimates for $\bm{\beta}$ and any nuisance parameters. In some cases, e.g. $Y|(X, \bm{C})$ normal and $X|(W,\bm{C})$ normal, one can obtain a closed form for the likelihood function. In other cases, the unobserved $X$'s in main study observations have to be integrated out numerically.

\subsubsection{\textcolor{red}{[Note: Section on pseudo-likelihood and quasi-likelihood?]}}

\subsubsection{Standard errors}

After obtaining $\hat{\bm{\beta}}$, one can estimate the variance-covariance matrix, $\hat{V}(\hat{\bm{\beta}})$, via the inverse of the Hessian of the log-likelihood function evaluated at $\hat{\bm{\beta}}$. Standard errors are taken as the square roots of the diagonal elements of $\hat{V}(\hat{\bm{\beta}})$.


\subsection{Measurement error methods - Regression calibration}

There are two procedures commonly referred to as regression calibration. In main study/external validation study designs where the disease model is any generalized linear model and there are as many surrogates as error-prone covariates, the two procedures give identical effect estimates \citep{thurston2003equivalence}. We term the two variants the ``conditional expectation'' view and the ``algebraic'' view of regression calibration \citep{rosner1989correction}. In the description that follows $X$ is taken to be scalar, but regression calibration generalizes to multiple error-prone covariates and surrogates \citep{rosner1990correction}.

For notation, a disease model of interest is a GLM that relates an outcome $Y$ to the error-prone $X$ and a (p x 1) covariate vector $\bm{C}$:
%
\begin{equation}
g[E(Y)] = \beta_0 + \beta_X X + \bm{\beta_C}^T \bm{C}
\end{equation}
%
A linear measurement error model relates $X$ to a surrogate $W$ and $\bm{C}$:
%
\begin{equation}
E(X) = \alpha_0 + \alpha_W W + \bm{\alpha_C}^T \bm{C}
\end{equation}
%
A notable advantage of regression calibration over maximum likelihood is that it does not require specifying the distribution of errors in the measurement error model.

% \subsubsection*{Scenario}
% 
% Suppose one wishes to fit a model relating an outcome $Y$ to a continuous exposure $X$ and covariates $\bm{C}$, but a surrogate $W$ rather than $X$ is observed in the main study. The goal is to estimate the regression parameters $\bm{\beta} = (\beta_0, \beta_X, \bm{\beta_C}$. An external validation dataset with $(X, W, \bm{C})$ is available, allowing one to fit the linear measurement error model: $X = \alpha_0 + \alpha_W W + \epsilon, \epsilon \sim (0, \sigma^2)$.

\subsubsection{Conditional expectation view}

In this implementation, one simply fits the disease model with $E(X|W, \bm{C}; \ \hat{\bm{\alpha}})$ in place of the unobserved $X$'s. External validation data is used to obtain $\hat{\bm{\alpha}}$ via OLS, and main study data is used to fit $Y$ vs. $E(X|W, \bm{C}; \ \hat{\bm{\alpha}})$ and $\bm{C}$. Resampling methods are often used to obtain standard errors and confidence intervals \citep{carroll2006measurement}.

\subsubsection{Algebraic view}

In this implementation, a ``naive'' version of the disease model, with $W$ in place of $X$, is introduced.
%
\begin{equation}
g[E(Y)] = \beta_0^* + \beta_X^* W + \bm{\beta_C}^{*T} \bm{C}
\end{equation}
%
When $g(\cdot)$ is the identity link, there is a simple algebraic relationship between $\bm{\beta}^*$ and $\bm{\beta}$. To see what the parameters in the naive model represent:
%
\begin{equation}
\begin{split}
E(Y|W, \bm{C}) & = \beta_0 + \beta_X E(X|W, \bm{C}) + \bm{\beta_C}^T \bm{C} \\
 & = \beta_0 + \beta_X (\alpha_0 + \alpha_W W + \bm{\alpha_C}^T \bm{C}) + \bm{\beta_C}^T \bm{C} \\
 & = (\beta_0 + \beta_X \alpha_0) + (\beta_X \alpha_W) W + (\beta_X \bm{\alpha_C}^T + \bm{\beta_C}^T) \bm{C}
\end{split}
\end{equation}

The regression coefficient for $W$ in the naive model represents $\beta_X \alpha_W$ rather than $\beta_X$. With validation data, one can estimate $\alpha_W$ and obtain the regression calibration exposure estimate $\hat{\beta}_X = \frac{\hat{\beta}_X^*}{\hat{\alpha}_W}$. Using the fact that different observations are used to estimate $\hat{\bm{\beta}}^*$ and $\hat{\bm{\alpha}}$, a Delta method variance estimator is given by $\frac{1}{\hat{\alpha}_W^2} \hat{V}(\hat{\beta}_X^*) + \frac{\hat{\beta}_X^{*2}}{\hat{\alpha}_W^4} \hat{V}(\hat{\alpha}_W)$.

Often the entire $\bm{\beta}$ vector is of interest, not just $\beta_X$. Looking at what each of the naive regression coefficients represent:
%
\begin{equation}
\begin{split}
\beta_0^* & = \beta_0 + \beta_X \alpha_0 \\
\beta_X^* & = \beta_X \alpha_W \\
\bm{\beta_C}^{*T} & = \beta_X \bm{\alpha_C}^T + \bm{\beta_C}^{*T}
\end{split}
\end{equation}
%
As a system of equations:
%
\begin{equation}
\left( \begin{array}{ccc} 1 & \alpha_0 & \bm{0}_p^T \\
                          0 & \alpha_W & \bm{0}_p^T \\
                          \bm{0}_p & \bm{\alpha_C} & I_p \end{array} \right)
\left( \begin{array}{c} \beta_0 \\ \beta_X \\ \bm{\beta_C} \end{array} \right) =
\left( \begin{array}{c} \beta_0^* \\ \beta_X^* \\ \bm{\beta_C}^* \end{array} \right) \qquad \text{or} \qquad \bm{A} \bm{\beta} = \bm{\beta}^*
\end{equation}

The $\bm{A}$ matrix is square and typically invertible, giving the regression calibration estimator $\hat{\bm{\beta}} = \hat{\bm{A}}^{-1} \bm{\hat{\beta}}^* = g(\hat{\bm{\beta}}^*, \hat{\bm{\alpha}})$. A variance estimator can be obtained using the Delta method and the fact that $V(\bm{\hat{\beta}}^*, \bm{\hat{\alpha}})$ is a block-diagonal matrix with $V(\bm{\hat{\beta}}^*)$ and $V(\bm{\hat{\alpha}})$ on the diagonal.

There are several ways to motivate a similar result for logistic regression \citep{rosner1989correction}, where the regression calibration estimator is approximately unbiased for $\beta_X$ when either (1) $\beta_X^2 V(X|W,\bm{C})$ is small (e.g. $<$ 0.5) or (2) the disease is rare and $X|W,\bm{C}$ is normally distributed \citep{kuha1994corrections}.

\subsubsection{Properties}

\textcolor{red}{Note: Need to add? Not sure.}

\subsubsection{Extensions}

%\citet{rosner1990correction} generalize the procedure to allow for a vector of error-prone covariates $\bm{X}$ and a corresponding vector of surrogates $\bm{Z}$, where there exactly one surrogate for each error-prone variable. A multivariate MEM relates $\bm{X}$ to $\bm{Z}$ and any precisely measured TDM covariates.

\citet{kuha1994corrections} developed a regression calibration estimator for logistic regression based on the same Taylor series expansion as in \citet{rosner1989correction}, but keeping the second-order term rather than dropping it. They demonstrated that this modified regression calibration estimator performs better than the usual one, particularly when the aforementioned requirements are not satisfied.

\citet{weller2007regression} extend regression calibration to logistic regression where there are multiple surrogates for a single error-prone exposure, again assuming one of the two assumptions given by \citet{kuha1994corrections} holds. In this scenario, one obtains an approximately unbiased estimate of the exposure effect for each surrogate, and the final estimate is a weighted linear combination of these estimates based on generalized least squares theory. \citet{kipnis2012regression} modified the approach to make it invariant to reparameterizations of nuisance parameters, but concluded that the usual regression calibration estimator, where the disease model is fit to main study data with the unobserved $X$ replaced by its expectation given all surrogates, performs at least as well as the two-stage procedures.

When validation data are internal, a natural idea is to fit the disease model using the measured $X$'s for internal validation observations and $E(X|W, \bm{C}; \ \hat{\bm{\alpha}})$ for main study participants. This is inefficient because it treats the exposure the same whether it is observed or imputed. \citet{spiegelman2001efficient} suggest a more efficient method along the lines of \citet{greenland1988variance}. Briefly, one uses the internal validation data to obtain an exposure effect estimate $\hat{\beta}_X^I$ and $\hat{V}(\hat{\beta}_X^I)$. Then, one ignores $Y$ in internal validation observations, treating it like external validation, and uses the usual regression calibration procedure to obtain $\hat{\beta}_C^{RC}$ and $\hat{V}(\hat{\beta}_X^{RC})$. Using the fact that the two estimators are asymptotically uncorrelated, the final estimator is given by $\hat{\beta}_X^{RC, I} = w_{RC} \hat{\beta}_X^{RC} + (1 - w_{RC}) \hat{\beta}_X^I$ where $w_{RC} = \frac{\hat{V}(\hat{\beta}_X^I)}{\hat{V}(\hat{\beta}_X^I) + \hat{V}(\hat{\beta}_X^{RC})}$.

Spiegelman's approach for internal validation data requires that $\beta_X$ can be estimated with main study/external validation data, which is true when $W$ is available and is a surrogate. But an advantage of having internal validation data is that it allows for $W$ to not be a surrogate for $X$, i.e. $f(y|x, w, \bm{c}) \neq f(y|x, \bm{c})$. In fact it allows for no surrogate $W$ to be observed at all. To our knowledge, an efficient version of regression calibration when validation data are internal and surrogacy does not hold has not been described to date.

\subsection{Maximum Likelihood and Regression Calibration Applied to Unmeasured Confounding} \label{ML/RC Applied to UC}

These methods apply directly to the unmeasured confounding scenario, with two main differences: (1) The covariate not measured precisely in the main study is not the primary exposure of interest $X$, but another covariate $Z$; and (2) There is no ``imprecise version'' of $Z$ available in the main study.

Consider again disease model \ref{tdm1} (Section \ref{Motivation}, p. \pageref{tdm1}), for a scalar unmeasured confounder $Z$:
%
\begin{equation*}
g[E(Y)] = \beta_0 + \beta_X X + \bm{\beta_C}^T \bm{C} + \beta_Z Z 
\end{equation*}
%
In regression calibration modeling, we specify a model for the missing $Z$ conditional on the other TDM covariates $(X, \ \bm{C})$ and perhaps additional covariates $\bm{D}$. For example:
%
\begin{equation*}
E(Z) = \beta_0 + \beta_X X + \bm{\beta_C}^T \bm{C} + \bm{\beta_D}^T \bm{D}
\end{equation*}
%
In the measurement error context, the imprecise version of $X$ ($W$ in the previous section) serves as $\bm{D}$; $W$ typically strongly informs $X$. Here, we rely on whatever covariance exists between $Z$ and $(X, \ \bm{C})$, adding in extra variables $\bm{D}$ that inform $Z$ if possible. 

In general, we might expect a relatively lower measurement error model $R^2$ in the unmeasured confounding setting than in the measurement error setting, because there is no imprecise version of $Z$. When the TDM is logistic regression, the regression calibration approximation may be more likely to break down in the unmeasured confounding setting, as $\beta_Z^2 V(Z|X,\bm{C},\bm{D})$ becomes large due to a large $V(Z|X,\bm{C},\bm{D})$. Assuming both the TDM and MEM are correctly specified, maximum likelihood should still perform adequately, provided numerical integration of $X$ out of the likelihood function is performed accurately.

Perhaps a more important problem stemming from the absence of an imprecise version of $Z$ is potential non-identifiability if validation data are external. If no variables $\bm{D}$ can be identified that inform $Z$ but not $Y$ given $(X, \ \bm{C})$ (and that are measured in both the main study and validation study), regression calibration will not produce unique estimates of $\bm{\beta}$. Taking the conditional expectation view of regression calibration, perfect collinearity between $E(Z|X, \bm{C}; \ \hat{\bm{\alpha}})$ and $(X, \ \bm{C})$ precludes fitting the TDM with these variables as predictors. The parameters will generally be identifiable using maximum likelihood as long as the TDM and MEM are not both normal linear models, but identifiability may be so slight that estimates remain unstable for feasible sample sizes \citep{carroll2006measurement}.

With internal validation data, regression calibration can be applied even with no $\bm{D}$, by fitting the TDM with the observed $Z$'s for validation study participants and $E(Z|X, \bm{C}; \ \hat{\bm{\alpha}})$ for main study participants. Maximum likelihood can also be applied, constructing the likelihood as shown in Equation \ref{ml_3types} (p. \pageref{ml_3types}).

\subsection{Propensity score calibration} \label{Propensity score calibration}

\subsubsection{Overview}

\citet{sturmer2005adjusting} developed a method called propensity score calibration to correct for unmeasured confounding with a main study/external validation study design. The basic idea is to control for confounding via a propensity score, i.e. the estimated probability of exposure given all covariates of interest. Because one or more covariates assumed to be confounders are not available, one can only estimate an error-prone propensity score, or the probability of exposure given the observed covariates. 

With validation data, one can estimate both the error-prone propensity score, $e(\bm{X}_{EP})$, a gold standard propensity score based on all covariates, $e(\bm{X}_{GS})$, and fit a linear model relating $e(\bm{X}_{GS})$ to the exposure and $e(\bm{X}_{EP})$. In the spirit of regression calibration, the estimated error-prone propensity scores for main study participants (obtained by re-fitting the error-prone propensity score model to main study data) are mapped to expected gold standard propensity scores. Then, one can use the ``conditional expectation'' view or the ``algebraic view'' of regression calibration to estimate the exposure effect. An advantage of the conditional expectation perspective is that it allows for the use of matching or stratification rather than covariate adjustment for the expected $e(\bm{X}_{GS})$ \citep{sturmer2007performance}.

\textcolor{red}{Note: See Bob's comment on p. 17. Can decide whether it's worth going into detail on which approach is best. Seems it depends on relative sizes of main study and validation study}

% In addition to covariate adjustment, stratification, and matching, a fourth application of the propensity score is inverse probability of treatment weighting (IPTW) \citep{austin2015moving}. Several IPTW estimators are available to estimate the average treatment effect (ATE), or the...

Propensity score calibration is simple to apply and particularly appealing when there are several unmeasured confounders. However, it only works with a binary exposure, and it does not produce regression coefficient estimates for other individual predictors (like propensity score analysis in general). 

\subsubsection{A different type of surrogacy}

Propensity score calibration holds an interesting advantage over regression calibration and the two-model maximum likelihood approach with regression calibration modeling in the unmeasured confounding setting. When validation data are external, the other methods require an extra predictor in the MEM that does not appear in the TDM. This is necessary for identifiability in regression calibration and stability in maximum likelihood, as described in Section \ref{ML/RC Applied to UC} (p. \pageref{ML/RC Applied to UC}).

\textcolor{red}{Note: Consider adding work on RC optimization for internal validation with no D. Also, Bob made note about prior M-estimation work}

Such a variable is called a surrogate for the true covariate, and omission of the surrogate from the TDM reflects the assumption that measurement error is non-differential. This is not a problem in the measurement error setting, where an imprecise version of the error-prone covariate is available for main study participants, and it is reasonable to assume that the error-prone version does not further inform the outcome given the true covariate. But in the unmeasured confounding setting, applying regression calibration or maximum likelihood requires carefully selecting at least one variable available in both the main study and validation study that informs the missing confounder, but does not inform the outcome given the other TDM covariates. 

Validity of propensity score calibration does depend on a surrogacy assumption, but it is a somewhat different type of surrogacy than what is typically assumed in regression calibration modeling. Consider application of the method with a linear TDM:
%
\begin{equation}
\begin{split}
\text{TDM:} & \quad Y = \beta_0 + \beta_1 X + \beta_2 e(\bm{X}_{GS}) + \epsilon, \ \epsilon \sim (0, \ \sigma^2) \\
\text{MEM:} & \quad E[e(\bm{X}_{GS})] = \lambda_0 + \lambda_X X + \lambda_P e(\bm{X}_{EP})
\end{split}
\end{equation}
%\vspace{4pt}
%
Omission of a $\beta_3 e(\bm{X}_{EP})$ term from the TDM reflects the assumption that $e(\bm{X}_{EP})$ does not inform $Y$ conditional on $(X, \ e(\bm{X}_{GS}))$. A lack of correlation between $X$ and $e(\bm{X}_{EP})$ would also ensure validity, but it is hard to imagine a case where an exposure would be uncorrelated with the probability of exposure given a (partial) covariate vector $\bm{X}_{EP}$. In that case, where all informative covariates are unmeasured in the main study, one might anticipate propensity score calibration breaking down entirely.

As for intuition on the surrogacy assumption, \citet{sturmer2007performance} state that ``surrogacy is violated when the direction of confounding in the exposure-disease association caused by the unobserved variable(s) differs from that of the confounding due to observed variables.'' 

When validation data are internal, the surrogacy assumption can be directly tested (e.g. fit the TDM with an extra $\beta_3 e(\bm{X}_{EP})$ term and test whether $\beta_3 = 0$) \citep{sturmer2005adjusting, sturmer2007performance}. If violated, one can allow for non-surrogacy, for example by fitting the TDM with both propensity scores, using the expected $e(\bm{X}_{GS})$ for main study participants and the estimated $e(\bm{X}_{GS})$ based on the full covariate vector $\bm{X}_{GS}$ for validation study participants.

%That would occur if the main study covariates did not inform $X$... What if you change the method to separate $\bm{X}_{GS}$ into two parts $\bm{X}_1$ which informs $X$ and $\bm{X}_2$ which does not. Problem is you need full $\bm{X}_2$ vector in main study...


\subsubsection{Standard errors}

\citet{sturmer2005adjusting} suggest using SAS macros for regression calibration developed by \citet{rosner1990correction} to obtain point estimates and standard errors for propensity score calibration. This approach does not take into account the fact that the propensity scores are estimated rather than known. Bootstrap standard errors and confidence intervals may be a useful alternative.




\newpage

\section{Results}

In this section we perform simulations to assess validity and efficiency of various corrective methods for unmeasured confounding scenarios with main study/validation study designs. All simulation scenarios involve a binary exposure $X$, one continuous or binary unmeasured confounder $Z$, and a continuous or binary outcome $Y$. Primary interest is in estimation of the $X-Y$ association controlling for $Z$ and potentially other confounders $\bm{C}$. Simulations were performed in R version 3.2.0 \citep{baser}. 

\subsection{Estimators}

The estimators of interest are described in Table \ref{uc_estimators}. The first four are corrective methods, and the last four are for comparative purposes.

\vspace{0.25in}
\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
\caption{Description of estimators for unmeasured confounding simulations.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Estimator} & \textbf{Description} \\
\midrule
$\hat{\beta}_{X, ML2}$ & Maximum likelihood based on regression calibration modeling (2 models) \\
$\hat{\beta}_{X, ML3}$ & Maximum likelihood based on classical modeling (3 models) \\
$\hat{\beta}_{X, RC}$ & Regression calibration \\
$\hat{\beta}_{X, PSC}$ & Propensity score calibration \\
\midrule
$\hat{\beta}_{X, C}$ & Complete data estimator from main study fit of $Y$ vs. truly unobserved $Z$ and $\bm{C}$ \\
$\hat{\beta}_{X, I}$ & Internal validation estimator (where possible) \\
$\hat{\beta}_{X, CR}$ & Crude estimator from main study fit of $Y$ vs. $X$ \\
$\hat{\beta}_{X, N}$ & Naive estimator from main study fit of $Y$ vs. $(X, \ \bm{C})$
\bottomrule
\end{tabular}
\label{uc_estimators}
\end{table}
\vspace{0.15in}

\subsubsection{Maximum likelihood implementation}

In all scenarios considered, (1) or (2) is the true ML, while the other is based on models that are incorrect but perhaps defensible given validation data. For example, the joint distribution of a binary exposure $X$ and continuous unmeasured confounder $Z$ may be generated as $Z \sim N(\cdot)$ and $X|Z \sim \text{Bern}(\cdot)$, but ML based on a normal linear model for $Z|X$ may appear reasonable.

% Regression calibration requires the unmeasured confounder be continuous, and propensity score calibration requires a binary exposure. Therefore we mostly focus on the scenario of a binary exposure and a continuous unmeasured confounder.

Likelihood functions were maximized using the R function \textit{nlminb}, which uses a quasi-Newton Raphson algorithm. Starting values were set to 0 for regression coefficients and 1 for variance terms; lower bounds for variance terms were set to 0. In cases where the unmeasured confounder had to be integrated out of the likelihood function numerically (3-model ML, 2-model ML with logistic TDM and linear MEM), the \textit{integrate} function was used. Variance-covariance matrices were estimated by inverting the Hessian matrix at the MLEs, which was approximated numerically via the \textit{hessian} function in the \textbf{numDeriv} package \citep{numderiv}.

In the 2-model ML scenario, when the TDM is logistic regression and the MEM is linear regression, likelihood contributions for main study participants are the logistic-normal integral \\ $\int_{-\infty}^\infty p_y^y (1 - p_y)^{1-y} \frac{1}{\sqrt{2 \pi \sigma_\delta^2}}e^{-\frac{1}{2 \sigma_\delta^2} (z-\mu_Z)^2} \ dz$, where $p_y = (1 + e^{-\beta_0 - \beta_Z z - \beta_X x - \bm{\beta_C}^T \bm{c}})^{-1}$ and $\mu_Z = \alpha_0 + \alpha_d d + \alpha_x x + \bm{\alpha_c}^T \bm{c}$. The integral represents $P(Y=y|x, \bm{c}, d)$. In simulations, we use the probit approximation $P(Y = 1|x, \bm{c}, d) \approx \frac{e^t}{1 + e^t}$ where $t = \frac{\beta_0 + \beta_Z \mu_Z + \bm{\beta_C}^T \bm{c}}{\sqrt{1 + \frac{\beta_Z^2 \sigma_\delta^2}{1.7^2}}}$. This approach has been used by other authors, with favorable results \citet{lyles2013approximate, carroll2006measurement}. %In simulations, we denote this approximate ML estimator as $\beta_{X, ML2, A}$.

%In simulations, we replace the logistic-normal integral with the probit approximation $\frac{e^t}{1 + e^t}$ where $t = \frac{\beta_0 + \beta_Z \mu_Z + \bm{\beta_C}^T \bm{c}}{\sqrt{1 + \frac{\beta_Z^2 \sigma_\delta^2}{1.7^2}}}$. This approach has been used by authors, with favorable results \citet{lyles2013approximate, carroll2006measurement}. In simulations, we denote this approximate ML estimator as $\beta_{X, ML2, A}$.

\subsubsection{Regression calibration implementation}

In scenarios involving a surrogate $D$ in the MEM but not the TDM, the algebraic view of RC was used. When validation data were internal, we used the weighted approach described previously \citep{spiegelman2001efficient}. In both cases, variance estimates were obtained via the Delta method.

When validation data were internal but there was no surrogate, we used the conditional expectation view of RC, fitting the TDM with observed $Z$'s for validation study participants and imputed $Z$'s, or $E(Z|X, \bm{C}; \hat{\bm{\alpha}})$, for main study participants. %Standard errors and percentile-based confidence intervals were obtained via bootstrapping (1,000 samples with replacement from main study and validation study).

When $Z$ was binary, we used the conditional expectation view, but with a logistic regression rather than linear regression model for $Z$ given measurement error covariates. Estimated probabilities were used in place of the unobserved $Z$ values.

\subsubsection{Propensity score calibration implementation}

In external validation scenarios, PSC was implemented as described by \citet{sturmer2005adjusting}. In internal validation scenarios, we included two versions, one with the usual surrogacy assumption and one with the surrogacy assumption relaxed. %When assuming surrogacy, the situation mirrors RC with internal validation data and a surrogate. Thus, we introduced a weighted PSC estimator in the same spirit as the weighted RC estimator.
In scenarios involving a surrogate $D$, we included two versions of PSC, one that did not incorporate $D$ into either propensity score, and one that included $D$ in both.

%In the 2-model ML scenario, when the TDM is logistic regression and the MEM is linear regression, main study likelihood contributions are $\int_{-\infty}^\infty f(y_i|z_i, x_i, \bm{c}_i) \cdot f(z_i|d_i, x_i, \bm{c}_i) \ dz_i$.involve the logistic-normal integral 


\subsection{Linear regression simulations}

\subsubsection{Scenario 1}

First we consider estimation of the association between a binary exposure $X$ and a continuous outcome $Y$ in the presence of confounding due to a continuous variable $Z$. The data generating process is summarized in Figure \ref{figure_linear} and Table \ref{table_linear}.

\vspace{0.1in}
\begin{figure}[h]
\centering
\raisebox{0.75in}{\includegraphics[height = 1.75in]{dags/linear.pdf}} \qquad \qquad
\raisebox{0in}{\includegraphics[height = 3in]{scatter_linear.pdf}}
\caption{DAG for Scenario 1 (left), and scatterplot showing relationships among variables in single trial with $n = 1,000$ (right). In scatterplot, dotted lines represent marginal means for $Y$ for each exposure group, and solid lines represent conditional means for $Y$ given $Z$.}
\label{figure_linear}
\end{figure}


\begin{table}[h]
\caption{Data generating process for Scenario 1.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Model} & \textbf{Parameter values} \\
\midrule

$Z \sim N(\mu_Z, \ \sigma_Z^2)$ & $\bm{\theta}_Z = (\mu_Z, \ \sigma_Z^2)^T = (0, \ 1)^T$ \\
$\text{logit}[P(X = 1)] = \gamma_0 + \gamma_Z Z$ & 
  $\bm{\gamma} = (\gamma_0, \ \gamma_Z)^T = (0, \ \text{log}(3))^T$ \\
$Y = \beta_0 + \beta_Z Z + \beta_X X + \epsilon, \ \epsilon \sim N(0, \ \sigma_\epsilon^2)$ & 
  $\bm{\beta} = (\beta_0, \ \beta_Z, \ \beta_X)^T = (0.25, \ 1, \ 0.75)^T; \ \sigma_\epsilon^2 = 1$\\

\bottomrule
\end{tabular}
\label{table_linear}
\end{table}

\vspace{0.25in}


<<eval = F, echo = F, results = "asis", fig.height = 4, fig.width = 4>>=
set.seed(123)
betas <- c(0.25, 1, 0.75)
sigsq.e <- 1
gammas <- c(0, log(3))
omegas <- c(0, 1)
n <- 1000
z <- rnorm(n = n, mean = omegas[1], sd = sqrt(omegas[2]))
x <- rbinom(n = n, size = 1, prob = (1 + exp(-gammas[1] - gammas[2] * z))^(-1))
y <- betas[1] + betas[2] * z + betas[3] * x + rnorm(n = n, mean = 0, sd = sqrt(sigsq.e))
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "scatter_linear.pdf", height = 3.75, width = 4.25)
par(mar=c(4.1,4.1,3.1,2.1))
plot(z, y, col = ifelse(x == 1, "blue", "red"), main = "Y vs. Z by X", ylab = "Y", xlab = "Z", 
     cex = 0.5, cex.lab = 1, cex.main = 1)
fit <- lm(y ~ z + x)
summary(lm(y ~ x))
summary(lm(y ~ z + x))
points(x = range(z), y = fit$coef[1] + fit$coef[2] * range(z), type = "l", col = "red")
points(x = range(z), y = fit$coef[1] + fit$coef[2] * range(z) + fit$coef[3], type = "l", col = "blue")
points(x = range(z), y = rep(mean(y[x == 0]), 2), type = "l", col = "red", lty = 2)
points(x = range(z), y = rep(mean(y[x == 1]), 2), type = "l", col = "blue", lty = 2)
legend("bottomright", col = c("blue", "red"), legend = c("X = 1", "X = 0"), pch = 1, bg = "white", cex = 1, pt.cex = 0.65)
dev.off()

@
%' 
%' \newpage
%' 
%' % \begin{table}[h]
%' % %\caption{SUMMARY}
%' % \centering
%' % \begin{tabular}{ll}
%' % \toprule
%' % \textbf{Data type} & \textbf{Variables observed} \\
%' % \midrule
%' % 
%' % Main study & (Y_i, X_i), \ i = 1, ..., n_M$ \\
%' % External validation study & (X_j, Z_j), \ j = 1, ..., n_E$ \\
%' % Internal validation study & (Y_k, X_k, Z_k) \ k = 1, ..., n_I$\\
%' % 
%' % \bottomrule
%' % \end{tabular}
%' % \label{tab:xxx}
%' % \end{table}
%' 

Here, $Y$ could represent the log-transformed values of a continuous health measure. Both $X$ and $Z$ are positively associated with $Y$, and presence vs. absence of $X$ has the same effect on $E(Y)$ as a 0.75-SD increase in $Z$. There is positive confounding due to $Z$, i.e. the crude $X-Y$ association overestimates the causal effect of $X$ on $Y$. This scenario reflects fairly strong epidemiological effects ($R^2 \approx 0.59$). We consider both main study/external validation study designs and main study/internal validation study designs. 

Performance of the various estimators is shown in Table \ref{results_linear}. The 3-model ML estimator, $\beta_{X, ML3}$, is not included because it gave virtually identical estimates as the 2-model ML estimator, $\beta_{X, ML2}$, but took much longer to compute due to the numerical integration.

\vspace{0.25in}
<<eval = T, echo = F, results = "asis">>=
library("xtable")
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_table.rda")
linear.xtable <- xtable(linear.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 1, as a function of main study, external validation, and internal validation sample sizes (1,000 trials each). Estimators are as defined in Table \\ref{uc_estimators}. The true value is $\\beta_X$ is 0.75. Propensity score calibration can not be performed in this scenario.", label = "results_linear")
print(linear.xtable, hline.after = c(-1, 0, 9, 18), scalebox='0.8')
@
\vspace{0.15in}

When validation data were external, none of the corrective methods under study gave valid estimates of $\beta_X$. ML estimates were sensitive to starting values and generally failed to distinguish $\beta_0$, $\beta_Z$, and $\beta_X$.

For regression calibration, the measurement error model is $E(Z|X) = \alpha_0 + \alpha_X X$. In the conditional expectation view, one uses main study data to fit the TDM with imputed $Z$ values, i.e. a linear regression of $Y$ vs. $X$ and $E(Z|X; \ \hat{\bm{\alpha}})$. The design matrix is $\left( \begin{array}{ccc} \bm{1}_{n_m} & \bm{x} & \hat{\alpha}_0 \bm{1}_{n_m}+ \hat{\alpha}_X \bm{x} \end{array} \right)$, where $\bm{x}$ is a column vector of each participant's $X$ value. The third column is a linear combination of the first two, so the design matrix is less than full rank and $\bm{\beta}$ is not identifiable.

In attempting to apply propensity score calibration, we have $\bm{X}_{GS} = \left\{ Z \right\}$ and $\bm{X}_{EP} = \left\{ \cdot \right\}$. The gold standard propensity score is $e(\bm{X}_{GS}) = P(X = 1 | Z)$ and the error-prone propensity score is $e(\bm{X}_{EP}) = P(X = 1)$. We need to use the validation data to fit $E[e(\bm{X}_{GS})] = \lambda_0 + \lambda_X X + \lambda_P e(\bm{X}_{EP})$. But $e(\bm{X}_{EP})$ is just the sample prevalence of $X$ in the validation data. With the same value for all observations, one cannot estimate $\lambda_P$. If that term is omitted, main study observations with $X = 0$ get mapped to $E[e(\bm{X}_{GS})] = \lambda_0$ and those with $X = 1$ get mapped to $E[e(\bm{X}_{GS})] = \lambda_0 + \lambda_X$. With $X$ and $E[e(\bm{X}_{GS})]$ perfectly correlated, one can only fit the TDM with $E[e(\bm{X}_{GS})]$ omitted, which gives the crude estimate $\beta_{X, CR}$.

When validation data were internal, ML and RC gave valid estimates of $\beta_X$, but ML was more efficient. Notably, for a fixed internal validation sample size, the efficiency of ML improves steadily with increasing main study sample size, while the efficiency of RC does not (Figure \ref{figure_results_linear}). For a fixed main study sample size, RC was barely more efficient than the internal validation estimator, which does not use main study data at all.

<<eval = F, echo = F>>=

# Load data
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_sds.rda")

# First results figure, showing efficiency as function of n.m
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_results_linear.pdf", height = 4.5, width = 9)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(1, 2))
locs <- c(14, 17, 20)
plot(linear.sds[locs, "n_M"], linear.sds[locs, 5], lwd = 1.5, cex = 0.6, ylim = c(0, 0.12), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = expression(paste("Efficiency vs. ", n[M], " (", n[I], " = 500)", sep = "")),
     ylab = "SD (1,000 trials)", xlab = expression(n[M]))
axis(side = 1, at = linear.sds[locs, "n_M"])
axis(side = 2, at = seq(0, 0.12, 0.04))
points(linear.sds[locs, "n_M"], linear.sds[locs, 5], type = "b", lty = 2, pch = 16, cex = 0.6, col = "black")
points(linear.sds[locs, "n_M"], linear.sds[locs, 7], type = "b", pch = 16, cex = 0.6, col = "blue")
points(linear.sds[locs, "n_M"], linear.sds[locs, 8], type = "b", pch = 16, cex = 0.6, col = "red")
legend("bottomleft", horiz = F, pch = 16, cex = 1, col = c("black", "red", "blue"), y.intersp = 1.2,
       legend = c(expression(hat(beta)[X.I]), expression(hat(beta)[X.RC]), expression(hat(beta)[X.ML2])))
#legend("bottomleft", horiz = F, legend = c("Internal", "RC", "ML"), pch = 16, cex = 1, col = c("black", "red", "blue"))

locs <- 16: 18
plot(linear.sds[locs, "n_I"], linear.sds[locs, 5], lwd = 1.5, cex = 0.6, ylim = c(0, 0.24), yaxt = "n",
     type = "n", pch = 16, col = "black", lty = 2, xaxt = "n",
     main = expression(paste("Efficiency vs. ", n[I], " (", n[M], " = 500)", sep = "")),
     ylab = "SD (1,000 trials)", xlab = expression(n[I]))
axis(side = 1, at = linear.sds[locs, "n_I"])
axis(side = 2, at = seq(0, 0.24, 0.06))
points(linear.sds[locs, "n_I"], linear.sds[locs, 5], type = "b", lty = 2, pch = 16, cex = 0.6, col = "black")
points(linear.sds[locs, "n_I"], linear.sds[locs, 7], type = "b", pch = 16, cex = 0.6, col = "blue")
points(linear.sds[locs, "n_I"], linear.sds[locs, 8], type = "b", pch = 16, cex = 0.6, col = "red")
legend("bottomleft", horiz = F, pch = 16, cex = 1, col = c("black", "red", "blue"), y.intersp = 1.2,
       legend = c(expression(hat(beta)[X.I]), expression(hat(beta)[X.RC]), expression(hat(beta)[X.ML2])))
#legend("bottomleft", horiz = F, legend = c("Internal", "RC (cond. exp.)", "2-model ML"), pch = 16, cex = 1, col = c("black", "red", "blue"))
dev.off()
@

\begin{figure}[h]
\centering
\includegraphics[height = 3in]{figure_results_linear.pdf}
\caption{Efficiency of various estimators in Scenario 1 with internal validation data.}
\label{figure_results_linear}
\end{figure}





\subsubsection{Scenario 2}

Next we consider a similar scenario, but with an extra variable $D$ that informs $Z$ but is independent of $Y$ given $(X, \ Z)$. In measurement error terminology, $D$ is a surrogate for $Z$. The DAG is shown in Figure \ref{figure_linear_d}. 
%
\vspace{0.15in}
\begin{figure}[h]
\centering
\includegraphics[height = 1.75in]{dags/linear_d.pdf}
\caption{DAG for Scenario 2.}
\label{figure_linear_d}
\end{figure}
%
As in Scenario 1, control for $Z$ is sufficient. $D$ and $X$ could also be linked by a causal path from $D$ to $X$ (a causal path in the other direction would cause a cyclic $D-Z-X$ loop). If $D$ did have a causal effect on $X$, control for $Z$ would still be sufficient, as $Z$ intercepts both backdoor paths from $X$ to $Y$.

The surrogate $D$ is important because it allows for the maximum likelihood methods and regression calibration to restore validity even when validation data are external. Propensity score calibration is not exactly intended for this scenario, since there is only one confounder and it is missing in main study observations. But the procedure can be applied nonetheless, taking the gold-standard set of covariates as $(Z, \ D)$ and the error-prone set $D$. 

Data generation is summarized in Table \ref{table_linear_d}.

\vspace{0.1in}

\renewcommand{\arraystretch}{1.2}

\begin{table}[h]
\caption{Data generating process for Scenario 2.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Model} & \textbf{Parameter values} \\
\midrule

$D \sim N(\mu_D, \ \sigma_D^2)$ & $\bm{\theta}_D = (\mu_D, \ \sigma_D^2)^T = (0, \ 1)^T$ \\
$Z = \psi_0 + \psi_D D + \delta, \ \delta \sim N(0, \ \sigma_\delta^2)$ & $\bm{\psi} = (\psi_0, \ \psi_D)^T = (0, \ 0.5)^T; \ \sigma_\delta^2 = 0.75$ \\
$\text{logit}[P(X = 1)] = \gamma_0 + \gamma_Z Z + \gamma_C C$ & 
  $\bm{\gamma} = (\gamma_0, \ \gamma_Z)^T = (0, \ \text{log}(3))^T$ \\
$Y = \beta_0 + \beta_Z Z + \beta_X X + \epsilon, \ \epsilon \sim N(0, \ \sigma_\epsilon^2)$ & 
  $\bm{\beta} = (\beta_0, \ \beta_Z, \ \beta_X)^T = (0.25, \ 1, \ 0.75)^T; \ \sigma_\epsilon^2 = 1$\\

\bottomrule
\end{tabular}
\label{table_linear_d}
\end{table}

\vspace{0.2in}

The conditions are virtually the same as in Scenario 1. The parameter values from Scenario 1 are unchanged, and $Z$ again has mean 0 and variance 1. The correlation between $D$ and $Z$ is 0.5. Performance of the various estimators is summarized in Table \ref{results_linear_d}.

<<eval = T, echo = F, results = "asis">>=
library("xtable")
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_d_table.rda")
linear.d.table <- linear.d.table[, -c(7, 10, 11, 13)]
linear.d.xtable <- xtable(linear.d.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 2, as a function of main study, external validation, and internal validation sample sizes (1,000 trials each). Estimators are as defined in Table \\ref{uc_estimators}. The true value is $\\beta_X$ is 0.75.", label = "results_linear_d")

print(linear.d.xtable, hline.after = c(-1, 0, 9, 18), scalebox='0.8')

# load("linear_d.rda")
# psc.results <- linear.d$psc.surr.d[[7]][, 2]
# hist(psc.results, breaks = 20)
# mean(psc.results)
# median(psc.results)
# ml.results <- linear.d[["ml2"]][[7]][, 2]
# hist(ml.results)
# 
# results <- linear.d$rc[[2]][, 2]
# hist(results, breaks = 20)
# mean(results)
# median(results)
# ml.results <- linear.d[["ml2"]][[7]][, 2]
# hist(ml.results)
# 
# sd(linear.d$psc.surr.d[[16]][, 2])
# sd(linear.d$psc.nosurr.d[[16]][, 2])
# plot(linear.d$psc.surr.d[[16]][, 2], linear.d$psc.nosurr.d[[16]][, 2])
@


With external validation data, ML and RC gave valid estimates of $\beta_X$, with very similar efficiency; in fact, they produced virtually identical point estimates in the majority of trials. PSC often produced extreme estimates on the low end when $n_E = 100$. For $n_E > 100$, extreme values did not occur, but the estimator exhibited downward mean (and median) bias and was sometimes left-skewed.
%The various estimators were approximately normally distributed in most scenarios, with the exception of $n_M = 1,000$ and $n_E = 100$, where they all exhibited some left skew (Figure \ref{histograms_linear_d}).

When validation data were internal, ML and RC were again valid, with ML having a small efficiency advantage. PSC was valid whether the surrogacy assumption was imposed or relaxed; the latter case was slightly more efficient, with standard deviations often similar to ML and RC. 

%Similarly, two versions of PSC were included for the internal validation scenario. Data on $(Y, X, e(\bm{X}_{GS}), e(\bm{X}_{EP}))$ were available for internal validation subjects, making it possible to relax the surrogacy assumption and include $e(\bm{X}_{EP})$ in the disease model fit. Both versions were approximately unbiased, but allowing for non-surrogacy seemed to improve stability, and gave $\beta_X$ estimates that were about as efficient as weighted RC and ML.

Figure \ref{figure_results_linear_d} compares efficiency of the valid estimators. In the internal validation scenario, PSC with the surrogacy assumption had the poorest performance, and was less efficient than the internal validation estimator in several scenarios. Among the other estimators, ML was the best performer, followed by the weighted RC estimator and then PSC with the surrogacy assumption relaxed.

<<eval = F, echo = F>>=

# Load data
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_d_sds.rda")

# First results figure, showing efficiency as function of n.m
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_results_linear_d.pdf", height = 8, width = 8)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(2, 2))
locs <- c(2, 5, 8)
plot(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 5], lwd = 1.5, cex = 0.6, ylim = c(0, 0.45), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = expression(paste("Efficiency vs. ", n[M], " (", n[E], " = 500)", sep = "")),
     ylab = "SD (1,000 trials)", xlab = expression(n[M]))
axis(side = 1, at = linear.d.sds[locs, "n_M"])
axis(side = 2, at = seq(0, 0.45, 0.15))
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 8], type = "b", pch = 16, cex = 0.6, col = "blue")
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 9], type = "b", pch = 16, cex = 0.6, col = "red")
legend("topright", horiz = F, legend = c(expression(hat(beta)[RC]), expression(hat(beta)[ML2])), 
       pch = 16, cex = 1, col = c("red", "blue"), y.intersp = 1.2)

locs <- 4: 6
plot(linear.d.sds[locs, "n_E"], linear.d.sds[locs, 5], lwd = 1.5, cex = 0.6, ylim = c(0, 0.35), yaxt = "n",
     type = "n", pch = 16, col = "black", lty = 2, xaxt = "n",
     main = expression(paste("Efficiency vs. ", n[E], " (", n[M], " = 500)", sep = "")),
     ylab = "SD (1,000 trials)", xlab = expression(n[E]))
axis(side = 1, at = linear.d.sds[locs, "n_E"])
axis(side = 2, at = seq(0, 0.4, 0.1))
# points(linear.d.sds[locs, "n_E"], linear.d.sds[locs, "Complete"], type = "b", pch = 16, cex = 0.6, col = "black")
points(linear.d.sds[locs, "n_E"], linear.d.sds[locs, 8], type = "b", pch = 16, cex = 0.6, col = "blue")
points(linear.d.sds[locs, "n_E"], linear.d.sds[locs, 9], type = "b", pch = 16, cex = 0.6, col = "red")
legend("topright", horiz = F, legend = c(expression(hat(beta)[RC]), expression(hat(beta)[ML2])), 
       pch = 16, cex = 1, col = c("red", "blue"), y.intersp = 1.2)

locs <- c(14, 17, 20)
plot(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 5], lwd = 1.5, cex = 0.6, ylim = c(0, 0.125), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = expression(paste("Efficiency vs. ", n[M], " (", n[I], " = 500)", sep = "")),
     ylab = "SD (1,000 trials)", xlab = expression(n[M]))
axis(side = 1, at = linear.d.sds[locs, "n_M"])
axis(side = 2, at = seq(0, 0.12, 0.04))
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 5], type = "b", lty = 2, pch = 16, cex = 0.6, col = "black")
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 8], type = "b", pch = 16, cex = 0.6, col = "blue")
#points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 10], type = "b", pch = 16, cex = 0.6, col = "red")
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 9], type = "b", pch = 16, cex = 0.6, col = "red")
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 11], type = "b", pch = 16, cex = 0.6, col = "purple")
points(linear.d.sds[locs, "n_M"], linear.d.sds[locs, 13], type = "b", pch = 16, cex = 0.6, col = "orange")
legend("bottomleft", horiz = F, pch = 16, cex = 1, col = c("purple", "black", "orange", "red", "blue"), y.intersp = 1.2,
       legend = c(expression(hat(beta)[PSC]),
                  expression(hat(beta)[I]),
                  expression(paste(hat(beta)[PSC], " (no surr.)", sep = "")),
                  expression(hat(beta)[RC]),
                  expression(hat(beta)[ML2])))

locs <- 16: 18
plot(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 5], lwd = 1.5, cex = 0.6, ylim = c(0, 0.3), yaxt = "n",
     type = "n", pch = 16, col = "black", lty = 2, xaxt = "n",
     main = expression(paste("Efficiency vs. ", n[I], " (", n[M], " = 500)", sep = "")),
     ylab = "SD (1,000 trials)", xlab = expression(n[I]))
axis(side = 1, at = linear.d.sds[locs, "n_I"])
axis(side = 2, at = seq(0, 0.3, 0.1))
points(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 5], type = "b", lty = 2, pch = 16, cex = 0.6, col = "black")
points(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 8], type = "b", pch = 16, cex = 0.6, col = "blue")
#points(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 10], type = "b", pch = 16, cex = 0.6, col = "red")
points(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 9], type = "b", pch = 16, cex = 0.6, col = "red")
points(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 11], type = "b", pch = 16, cex = 0.6, col = "purple")
points(linear.d.sds[locs, "n_I"], linear.d.sds[locs, 13], type = "b", pch = 16, cex = 0.6, col = "orange")
legend("topright", horiz = F, pch = 16, cex = 1, col = c("purple", "black", "orange", "red", "blue"), y.intersp = 1.2,
       legend = c(expression(hat(beta)[PSC]),
                  expression(hat(beta)[I]),
                  expression(paste(hat(beta)[PSC], " (no surr.)", sep = "")),
                  expression(hat(beta)[RC]),
                  expression(hat(beta)[ML2])))

dev.off()

@

\begin{figure}[h]
\centering
\includegraphics[height = 6.75in]{figure_results_linear_d.pdf}
\caption{Efficiency of various estimators in Scenario 2.}
\label{figure_results_linear_d}
\end{figure}

One reason that PSC was not valid in the external validation scenario may be that $D$ did not inform $X$ given $Z$, and thus $D$ was not useful in the gold standard propensity score fit for $P(X|Z, D)$. Incorporating $D$ into the data generation for $X$ (equivalently, adding an arrow from $D$ to $X$ on the DAG) did not result in valid PSC estimation, whether the association between $D$ and $X$ was positive or negative (not shown). % Could also mention that it made PSC with surrogacy invalid in internal val. scenario...

% Maybe comment that PSC also didn't work when there was an arrow from D to X.... : For PSC, the error-prone propensity score is $P(X|D)$ and the gold standard propensity score is $P(X|D, Z)$. But $X$ values were generated based on $Z$ only, not $D$. Thus gold given $D$, and the gold standard propensity score is the 

A natural question is how the efficiency of main study/validation data estimators compare to the hypothetical case where the missing confounder is available in main study participants. This is particularly important if an investigator is considering collecting validation data, i.e. it is not already available from a different study.

Intuitively, the better $D$ informs $Z$, the less is lost by not having $Z$ itself in the main study. In the previous scenario, $\text{Cor}(D, Z)$ was 0.5. Figure \ref{figure_results_linear_d_cor} compares the efficiency of the 2-model ML estimator to the complete data estimator in main study participants for $\psi_D$ values that produce $\text{Cor}(D, Z)$ values of 0.1, 0.5, and 0.9. Internal validation observations were much more valuable than external. Even when the correlation was 0.9, the main study/external validation estimator was much less efficient than the complete estimator.  %if the surrogate $D$ is extremely informative of the unmeasured confounder, a main study/external validation study will be considerably less efficient than if the unmeasured confounder was available in the main study.

<<eval = F, echo = F>>=

# Load data
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_d_cor_sds.rda")

# First results figure, showing efficiency as function of n.m
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_results_linear_d_cor.pdf", height = 9, width = 4.5)
par(mar=c(5.1,5.1,4.1,2.1))
par(mfrow = c(3, 1))
locs <- 1: 5
plot(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 5], lwd = 1.5, cex = 0.7, ylim = c(0, 0.65), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n", cex.lab = 1.1, cex.axis = 0.9,
     main = "Cor(D, Z) = 0.1", ylab = "SD (1,000 trials)", xlab = "Validation study sample size")
axis(side = 1, at = linear.d.cor.sds[locs, "n_E"], cex.lab = 1.1, cex.axis = 0.9)
axis(side = 2, at = seq(0, 0.6, 0.2), cex.lab = 1.1, cex.axis = 0.9)
points(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 8], type = "b", pch = 16, cex = 0.7, col = "blue")
points(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 5], type = "b", pch = 16, cex = 0.7, col = "black", lty = 2)
locs <- 6: 10
points(linear.d.cor.sds[locs, "n_I"], linear.d.cor.sds[locs, 8], type = "b", pch = 16, cex = 0.7, col = "red")
legend("topright", horiz = F, legend = c(expression(paste(hat(beta)[ML], " (External val.)", sep = "")),
                                         expression(paste(hat(beta)[ML], " (Internal val.)", sep = "")), 
                                         expression(hat(beta)[C])),
       pch = 16, cex = 0.9, col = c("blue", "red", "black"), y.intersp = 1.2)

locs <- 11: 15
plot(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 5], lwd = 1.5, cex = 0.7, ylim = c(0, 0.3), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n", cex.lab = 1.1, cex.axis = 0.9,
     main = "Cor(D, Z) = 0.5", ylab = "SD (1,000 trials)", xlab = "Validation study sample size")
axis(side = 1, at = linear.d.cor.sds[locs, "n_E"], cex.lab = 1.1, cex.axis = 0.9)
axis(side = 2, at = seq(0, 0.3, 0.1), cex.lab = 1.1, cex.axis = 0.9)
points(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 8], type = "b", pch = 16, cex = 0.7, col = "blue")
points(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 5], type = "b", pch = 16, cex = 0.7, col = "black", lty = 2)
locs <- 16: 20
points(linear.d.cor.sds[locs, "n_I"], linear.d.cor.sds[locs, 8], type = "b", pch = 16, cex = 0.7, col = "red")
legend("topright", horiz = F, legend = c(expression(paste(hat(beta)[ML], " (External val.)", sep = "")),
                                         expression(paste(hat(beta)[ML], " (Internal val.)", sep = "")), 
                                         expression(hat(beta)[C])),
       pch = 16, cex = 0.9, col = c("blue", "red", "black"), y.intersp = 1.2)

locs <- 21: 25
plot(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 5], lwd = 1.5, cex = 0.7, ylim = c(0, 0.3), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n", cex.lab = 1.1, cex.axis = 0.9,
     main = "Cor(D, Z) = 0.9", ylab = "SD (1,000 trials)", xlab = "Validation study sample size")
axis(side = 1, at = linear.d.cor.sds[locs, "n_E"], cex.lab = 1.1, cex.axis = 0.9)
axis(side = 2, at = seq(0, 0.3, 0.1), cex.lab = 1.1, cex.axis = 0.9)
points(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 8], type = "b", pch = 16, cex = 0.7, col = "blue")
points(linear.d.cor.sds[locs, "n_E"], linear.d.cor.sds[locs, 5], type = "b", pch = 16, cex = 0.7, col = "black", lty = 2)
locs <- 26: 30
points(linear.d.cor.sds[locs, "n_I"], linear.d.cor.sds[locs, 8], type = "b", pch = 16, cex = 0.7, col = "red")
legend("topright", horiz = F, legend = c(expression(paste(hat(beta)[ML], " (External val.)", sep = "")),
                                         expression(paste(hat(beta)[ML], " (Internal val.)", sep = "")), 
                                         expression(hat(beta)[C])),
       pch = 16, cex = 0.9, col = c("blue", "red", "black"), y.intersp = 1.2)

dev.off()

@

\begin{figure}[h]
\centering
\includegraphics[height = 7.75in]{figure_results_linear_d_cor.pdf}
\caption{Relationship between validation study sample size and efficiency of 2-model ML in Scenario 2, with $n_M = 1,000$. The correlation between $D$ and $Z$ was controlled by varying $\psi_D$.}
\label{figure_results_linear_d_cor}
\end{figure}


\clearpage

\subsubsection{Scenario 3}

In the third scenario, we remove the surrogate $D$ and add a confounder $C$ (Figure \ref{figure_linear_c}). Control for $(Z, \ C)$ is necessary to estimate the causal effect of $X$ on $Y$ in this scenario. 

\vspace{0.15in}
\begin{figure}[h]
\centering
\includegraphics[height = 2.25in]{dags/linear_c.pdf}
\caption{DAG for Scenario 3.}
\label{figure_linear_c}
\end{figure}
\vspace{0.1in}

When validation are external, we can expect ML and RC to have the same problems as in Scenario 1. PSC would seem to be well-suited here, as it was developed for the case where there are several covariates but only a subset are available in the main study. 

Data generation is summarized in Table \ref{table_linear_c}.

\vspace{0.2in}
\renewcommand{\arraystretch}{1.2}

\begin{table}[h]
\caption{Data generating process for Scenario 3.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Model} & \textbf{Parameter values} \\
\midrule

$C \sim N(\mu_C, \ \sigma_C^2)$ & $\bm{\theta}_C = (\mu_C, \ \sigma_C^2)^T = (0, \ 1)^T$ \\
$Z = \psi_0 + \psi_C C + \delta, \ \delta \sim N(0, \ \sigma_\delta^2)$ & $\bm{\psi} = (\psi_0, \ \psi_C)^T = (0, \ 0.5)^T; \ \sigma_\delta^2 = 0.75$ \\
$\text{logit}[P(X = 1)] = \gamma_0 + \gamma_Z Z + \gamma_C C$ & 
  $\bm{\gamma} = (\gamma_0, \ \gamma_Z, \ \gamma_C)^T = (0, \ \text{log}(3), \ \text{log}(2))^T$ \\
$Y = \beta_0 + \beta_Z Z + \beta_X X + \epsilon, \ \epsilon \sim N(0, \ \sigma_\epsilon^2)$ & 
  $\bm{\beta} = (\beta_0, \ \beta_Z, \ \beta_X)^T = (0.25, \ 1, \ 0.75)^T; \ \sigma_\epsilon^2 = 1$\\

\bottomrule
\end{tabular}
\label{table_linear_c}
\end{table}
\vspace{0.2in}

Performance of the various estimators is summarized in Table \ref{results_linear_c}.

<<eval = T, echo = F, results = "asis">>=
library("xtable")
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_c_table.rda")
linear.c.table <- linear.c.table[, -c(8, 12)]
linear.c.xtable <- xtable(linear.c.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 3, as a function of main study, external validation, and internal validation sample sizes (1,000 trials each). Estimators are as defined in Table \\ref{uc_estimators}. The true value is $\\beta_X$ is 0.75.", label = "results_linear_c")
print(linear.c.xtable, hline.after = c(-1, 0, 9, 18), scalebox='0.7')

# load("linear_c.rda")
# psc.results <- linear.c[["psc.surr"]][[7]][, 2]
# hist(psc.results)
# mean(psc.results)
# median(psc.results)
# hist(linear.d[["psc"]][[1]][, 2])

# setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
# load("linear_c2_table.rda")
# linear.c2.table <- linear.c2.table[, -5]
# linear.c2.xtable <- xtable(linear.c2.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 3 (1,000 trials). The true value is 0.75.", label = "results_linear_c")
# print(linear.c.xtable, scalebox='0.7')

@

As expected, ML and RC were biased when validation data were external.  It was unclear in 1,000 trials whether PSC was unbiased, as mean $\hat{\beta}_X$ ranged from 0.679 to 0.782. Even with 5,000 trials, means ranged from 0.667 to 0.778 (not tabulated). In a single large trial with $n_M = n_E = 1,000,000$, PSC gave $\hat{\beta}_X = 0.803$, suggesting some upward bias, but less than ML and RC.

With internal validation data, similar trends were observed as in Scenario 1. In subsequent simulations, changing $\beta_C$ from 0.5 to -1 resulted in PSC exhibiting worse bias than ML and RC in the external validation scenario (not shown).


<<eval = F, echo = F>>=

# Load data
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_c_sds.rda")

# First results figure, showing efficiency as function of n.m
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_results_linear_c.pdf", height = 4.5, width = 9)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(1, 2))

locs <- c(2, 5, 8)
plot(linear.c.sds[locs, "n_M"], linear.d.sds[locs, "Internal"], lwd = 1.5, cex = 0.6, ylim = c(0, 0.45), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = expression(paste("SD ", hat(beta)[X], " vs. ", n[M], " (", n[E], " = 500)", sep = "")),
     ylab = expression(paste("SD ", hat(beta)[X])), xlab = expression(n[M]))
axis(side = 1, at = linear.d.sds[locs, "n_M"])
axis(side = 2, at = seq(0, 0.45, 0.15))
points(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "PSC (surr.)"], type = "b", pch = 16, cex = 0.6, col = "red")
legend("bottomleft", horiz = F, legend = "PSC (surr.)", pch = 16, cex = 1, col = "red")

locs <- 4: 6
plot(linear.c.sds[locs, "n_E"], linear.c.sds[locs, "Complete"], lwd = 1.5, cex = 0.6, ylim = c(0, 0.5), yaxt = "n",
     type = "n", pch = 16, col = "black", lty = 2, xaxt = "n",
     main = expression(paste("SD ", hat(beta)[X], " vs. ", n[E], " (", n[M], " = 500)", sep = "")),
     ylab = expression(paste("SD ", hat(beta)[X])), xlab = expression(n[E]))
axis(side = 1, at = linear.c.sds[locs, "n_E"])
axis(side = 2, at = seq(0, 0.4, 0.1))
points(linear.c.sds[locs, "n_E"], linear.c.sds[locs, "PSC (surr.)"], type = "b", pch = 16, cex = 0.6, col = "red")
legend("bottomleft", horiz = F, legend = "PSC (surr.)", pch = 16, cex = 1, col = "red")


locs <- c(11, 14, 17)
plot(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "Internal"], lwd = 1.5, cex = 0.6, ylim = c(0, 0.125), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = expression(paste("SD ", hat(beta)[X], " vs. ", n[M], " (", n[I], " = 500)", sep = "")),
     ylab = expression(paste("SD ", hat(beta)[X])), xlab = expression(n[M]))
axis(side = 1, at = linear.c.sds[locs, "n_M"])
axis(side = 2, at = seq(0, 0.12, 0.04))
points(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "Internal"], type = "b", lty = 2, pch = 16, cex = 0.6, col = "black")
points(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "2-model ML"], type = "b", pch = 16, cex = 0.6, col = "blue")
points(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "RC (cond. exp.)"], type = "b", pch = 16, cex = 0.6, col = "red")
points(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "PSC (surr.)"], type = "b", pch = 16, cex = 0.6, col = "purple")
points(linear.c.sds[locs, "n_M"], linear.c.sds[locs, "PSC (no surr.)"], type = "b", pch = 16, cex = 0.6, col = "darkgreen")
legend("bottomleft", horiz = F, legend = c("PSC (surr.)", "Internal", "PSC (no surr.)", "RC (cond. exp.)", "RC (weighted)", "2-model ML"), 
       pch = 16, cex = 1, col = c("purple", "black", "darkgreen", "red", "darkred", "blue"))

locs <- 13: 15
plot(linear.c.sds[locs, "n_I"], linear.d.sds[locs, "Complete"], lwd = 1.5, cex = 0.6, ylim = c(0, 0.3), yaxt = "n",
     type = "n", pch = 16, col = "black", lty = 2, xaxt = "n",
     main = expression(paste("SD ", hat(beta)[X], " vs. ", n[I], " (", n[M], " = 500)", sep = "")),
     ylab = expression(paste("SD ", hat(beta)[X])), xlab = expression(n[I]))
axis(side = 1, at = linear.c.sds[locs, "n_I"])
axis(side = 2, at = seq(0, 0.3, 0.1))
points(linear.c.sds[locs, "n_I"], linear.c.sds[locs, "Internal"], type = "b", lty = 2, pch = 16, cex = 0.6, col = "black")
points(linear.c.sds[locs, "n_I"], linear.c.sds[locs, "2-model ML"], type = "b", pch = 16, cex = 0.6, col = "blue")
points(linear.c.sds[locs, "n_I"], linear.c.sds[locs, "RC (cond. exp.)"], type = "b", pch = 16, cex = 0.6, col = "red")
points(linear.c.sds[locs, "n_I"], linear.c.sds[locs, "PSC (surr.)"], type = "b", pch = 16, cex = 0.6, col = "purple")
points(linear.c.sds[locs, "n_I"], linear.c.sds[locs, "PSC (no surr.)"], type = "b", pch = 16, cex = 0.6, col = "darkgreen")
legend("topright", horiz = F, legend = c("PSC (surr.)", "Internal", "PSC (no surr.)", "RC (cond. exp.)", "RC (weighted)", "2-model ML"), 
       pch = 16, cex = 1, col = c("purple", "black", "darkgreen", "red", "darkred", "blue"))

dev.off()


@



\subsubsection{Scenario 4}

Finally, we consider a similar scenario as the previous, but with $Z$ binary rather than continuous. The DAG is the same as in Figure \ref{figure_linear_c}; data generation is summarized in Table \ref{table_linear_c_binaryz}. Performance of the various estimators is summarized in Table \ref{results_linear_c}. Interestingly, with a large enough sample size, the ML estimator produced valid estimates of $\beta_X$ even when there was no validation data.

\vspace{0.25in}
%\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
\caption{Data generating process for Scenario 4.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Model} & \textbf{Parameter values} \\
\midrule

$C \sim N(\mu_C, \ \sigma_C^2)$ & $\bm{\theta}_C = (\mu_C, \ \sigma_C^2)^T = (0, \ 1)^T$ \\
$\text{logit}[P(Z = 1)] = \psi_0 + \psi_C C$ & $\bm{\psi} = (\psi_0, \ \psi_C)^T = (-0.5, 1.25)^T$ \\
$\text{logit}[P(X = 1)] = \gamma_0 + \gamma_Z Z + \gamma_C C$ & 
  $\bm{\gamma} = (\gamma_0, \ \gamma_Z, \ \gamma_C)^T = (0, \ \text{log}(3), \ \text{log}(2))^T$ \\
$Y = \beta_0 + \beta_Z Z + \beta_X X + \epsilon, \ \epsilon \sim N(0, \ \sigma_\epsilon^2)$ & 
  $\bm{\beta} = (\beta_0, \ \beta_Z, \ \beta_X, \ \beta_C)^T = (0.25, \ 1, \ 0.75, \ 0.5)^T; \ \sigma_\epsilon^2 = 1$\\

\bottomrule
\end{tabular}
\label{table_linear_c_binaryz}
\end{table}
\vspace{0.2in}


<<eval = T, echo = F, results = "asis">>=
library("xtable")
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("linear_c_binaryz_table.rda")
linear.c.binaryz.table <- linear.c.binaryz.table[, -c(8, 12)]
linear.c.binaryz.xtable <- xtable(linear.c.binaryz.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 4, as a function of main study, external validation, and internal validation sample sizes (1,000 trials each). Estimators are as defined in Table \\ref{uc_estimators}. The true value is $\\beta_X$ is 0.75.", label = "results_linear_c3")
print(linear.c.binaryz.xtable, hline.after = c(-1, 0, 4, 16, 28), scalebox='0.7')

# load("linear_c.rda")
# psc.results <- linear.c[["psc.surr"]][[7]][, 2]
# hist(psc.results)
# mean(psc.results)
# median(psc.results)
# hist(linear.d[["psc"]][[1]][, 2])
@





% \begin{figure}[h]
% \centering
% \includegraphics[height = 6in]{figure_results_linear_d.pdf}
% \caption{Efficiency of various estimators in Scenario 2.}
% \label{figure_results_linear_d}
% \end{figure}

%' \subsubsection{Scenario 4}
%' 
%' This final scenario is the same as Scenario 3, but with $\beta_C$ changed from 0.5 to -1. Results are shown in Table \ref{results_linear_c3}.
%' 
%' <<eval = T, echo = F, results = "asis">>=
%' library("xtable")
%' setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
%' load("linear_c3_table.rda")
%' linear.c3.table <- linear.c3.table[, -5]
%' linear.c3.xtable <- xtable(linear.c3.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 4 (1,000 trials). The true value is 0.75.", label = "results_linear_c3")
%' print(linear.c3.xtable, scalebox='0.7')
%' 
%' # load("linear_c.rda")
%' # psc.results <- linear.c[["psc.surr"]][[7]][, 2]
%' # hist(psc.results)
%' # mean(psc.results)
%' # median(psc.results)
%' # hist(linear.d[["psc"]][[1]][, 2])
%' @
%' 
%' In this case, PSC's surrogacy assumption was violated; the direction of confounding due to $Z$ was different from the direction of confounding due to $C$. This resulted in PSC exhibiting the worst bias of all methods when validation data were external. As in previous scenarios, PSC performed relatively well when validation data were internal and the surrogacy assumption was relaxed. 



\subsection{Logistic regression motivating example and simulations}

For logistic regression, we start with a motivating example, and then perform simulations under conditions that mirror it.

The Effects of Aspirin in Gestation and Reproduction (EAGeR) Study was a clinical trial aimed at determining whether daily low-dose aspirin lowers the rate of pregnancy loss. A total of 1,228 women age 18-40 who had one or two prior miscarriages and planned to become pregnant again participated in the study. Further details are provided by \citet{schisterman2013randomised}.

The primary finding from EAGeR was that low-dose aspirin was not associated with live birth rate or pregnancy loss \citep{schisterman2014preconception}. But data from the trial has been used to explore a variety of other research questions as well: for example, whether leptin is associated with live birth rate \citep{zarek2015higher}, and whether C-reactive protein is associated with pregnancy loss \citep{mumford2015c}.

Suppose we are interested in testing the hypothesis that higher vitamin D levels are associated with higher odds of becoming pregnant, and we wish to control for maternal age, body mass index (BMI), and caloric intake. All of these variables except caloric intake are available in EAGeR. Data from a different study, BioCycle, can seve as an external validation sample. 

BioCycle was a longitudinal study on oxidative stress and hormone levels during the menstrual cycle. A total of 259 women age 18-44 participated. Because this validation data is external, we have to assume transportability, or that the relationships among variables in BioCycle participants are the same as in EAGeR participants. It is impossible to directly test this assumption, but we can compare some basic characteristics across the two samples (Table \ref{table_example1}). 

\vspace{0.15in}
<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/Document")
load("table_example1.rda")

#linear.c3.table <- linear.c3.table[, -5]
#linear.c3.xtable <- xtable(linear.c3.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in Scenario 4 (1,000 trials). The true value is 0.75.", label = "results_linear_c3")
print(table.example1, scalebox='0.9')

# load("linear_c.rda")
# psc.results <- linear.c[["psc.surr"]][[7]][, 2]
# hist(psc.results)
# mean(psc.results)
# median(psc.results)
# hist(linear.d[["psc"]][[1]][, 2])
@
\vspace{0.15in}

EAGeR participants were slightly older, much more likely to be white, and had a higher rate of obesity than BioCycle participants. EAGeR participants were also unique in that they were all trying to get pregnant, and all had one or two prior miscarriages.

In EAGeR, the crude odds ratio for low vitamin D (defined as $<$ 30 ng/ml) and incident pregnancy was 0.69 (95\% CI 0.53-0.91). There was a significant race-by-low vitamin D interaction on log-odds of pregnancy before (p = 0.028) and after (p = 0.021) controlling for age and obesity. The race-specific logistic regression fits are shown in Tables \ref{table_example1_white} and \ref{table_example1_nonwhite}. The results suggest that low vitamin D is associated with lower odds of pregnancy in white women but higher odds of pregnancy in non-white women. The non-white sample was somewhat small (n = 46; 27 pregnancies), and it seems very unlikely that the true odds ratio in this group could be so high. The opposite direction of association for low vitamin D and pregnancy rates observed here could be spurious, or could be driven by something like racial differences in dietary sources of vitamin D (e.g. some foods high in vitamin D are also high in toxins that may lower the odds of pregnancy).

<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/Document")
load("table_example1_white.rda")
print(table.example1.white, scalebox='0.9')
@


<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/Document")
load("table_example1_nonwhite.rda")
print(table.example1.nonwhite, scalebox='0.9')
@


Unfortunately, only a subset of BioCycle participants had data on caloric intake from 24-hour dietary recall, and all were white. Therefore we have to restrict the main study/external validation analysis to white women. 

For a surrogate that informs the unmeasured confounder, caloric intake, but is assumed independent of the outcome given other covariates, we use the fatty acid \textit{MUFA161}. It was measured in both studies, and was moderately correlated with caloric intake in BioCycle (r = 0.69, p $<$ 0.001). 

A total of 89 BioCycle participants had complete data on caloric intake (from several 24-hour dietary recalls) as well as vitamin D, \textit{MUFA161}, age, and obesity status. The measurement error model fit for this data is shown in Table \ref{table_biocycle_mem}. Residuals from this model were approximately normally distributed (Shapiro-Wilk p = 0.12). 

\vspace{0.2in}
<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/Document")
load("table_biocycle_mem.rda")
print(table.biocycle.mem, scalebox='0.9')
@
\vspace{0.1in}

Table \ref{table_eager_biocycle} shows the estimated regression coefficients for various corrective methods. The various methods gave virtually identical point estimates, suggesting women who are vitamin D deficient have 27-28\% lower odds of pregnancy than women who are not Vitamin D deficient. The suspected unmeasured confounder, caloric intake, did not appear to be related to odds of pregnancy.

\vspace{0.1in}
<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/Document")
load("table_eager_biocycle.rda")
#table.eager.biocycle <- table.eager.biocycle[, -2]
print(table.eager.biocycle, scalebox='0.9')
@
\vspace{0.1in}

\subsubsection{Simulation studies}

Data generation is summarized in Table \ref{table_logistic}. The degree of unmeasured confounding was controlled by varying $\beta_Z$ while also adjusting $\beta_0$ to maintain the pregnancy rate around 74\%.

\vspace{0.1in}

\renewcommand{\arraystretch}{1.2}

\begin{table}[h]
\caption{Data generating process for logistic regression simulations.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Models}\\
\midrule

$D = \textit{MUFA161} \sim N(0.48, \ 0.07)$  \\
$C_1 = \text{Age} \sim \text{Uni}(18, \ 40)$ \\
$C_2 = \text{Obesity} \sim \text{Bern}(0.2)$ \\
$X = \text{Vitamin D } < \text{30 ng/ml} \sim \text{Bern}[(1 + e^{0.09 - 0.23 D + 0.005 C_1 - 1.13 C_2})^{-1}]$ \\
$Z = 15 + 6.79 D - 2 X - 0.01 C_1 - 0.33 C_2 + \delta, \ \delta \sim N(0, \ 9.19)$ \\
$Y = \text{Pregnancy} \sim \text{Bern}[(1 + e^{-\beta_0 - \beta_Z Z + 0.33 X + 0.03 C_1 + 0.72 C_2})^{-1}]$ \\

\bottomrule
\end{tabular}
\label{table_logistic}
\end{table}

\vspace{0.2in}

Results are shown in Table \ref{results_logistic}. ML and RC were approximately mean unbiased for $0.91 \le OR_{ZY} \le 1.10$. Both estimators exhibited mean downward bias when $Z$ had a strong negative association with $Y$, and mean upward bias when $Z$ had a strong positive association with $Y$; the mean bias in ML was much worse. ML was more prone to extreme estimates than RC; when $OR_{ZY} = 0.67$, ML estimates for $\beta_X$ included -7.45 and -3.48, while all RC estimates were within (-1.64, 0.52).

PSC exhibited a great deal of upward mean bias when the association between $Z$ and $Y$ was inverse or null, and was clearly invalid for all scenarios except $OR_{ZY} = 1.10$.

<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims")
load("logistic_table.rda")
logistic.table <- logistic.table[, -c(4, 6, 9, 10)]
#colnames(logistic.table) <- c("$OR_{ZY}$", "Complete", "Naive", "2-model ML", "2-model ML (approx.)", "2-model ML (pseudo.)", "RC (algebraic)", "PSC (no D)", "PSC (D in EP)", "PSC (D in both)")

logistic.xtable <- xtable(logistic.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in logistic regression simulation study (1,000 trials). The true value is -0.33.", label = "results_logistic")
print(logistic.xtable, scalebox='0.8')

# load("logistic_med_table.rda")
# logistic.med.table <- logistic.med.table[, -c(4, 6, 9, 10)]
# logistic.med.xtable <- xtable(logistic.med.table, caption = "Median (IQR) of various estimators for $\\beta_X$ in logistic regression simulation study (1,000 trials). The true value is -0.33.", label = "results_logistic_med")
# print(logistic.med.xtable, scalebox='0.75')

# Create Figure showing median +/- IQR
# load("logistic.rda")
# or.z.vals <- c(1/2, 1/1.5, 1/1.1, 1, 1.1, 1.5, 2)
# medians <- matrix(unlist(lapply(logistic, function(x) lapply(x, function(x) apply(x, 2, function(x) median(x, na.rm = T))[2]))),
#                     nrow = length(or.z.vals), byrow = FALSE)[, -c(3, 5, 8, 9)]
# lowers <- matrix(unlist(lapply(logistic, function(x) lapply(x, function(x) apply(x, 2, function(x) quantile(x, probs = 0.25, na.rm = T))[2]))),
#                     nrow = length(or.z.vals), byrow = FALSE)[, -c(3, 5, 8, 9)]
# uppers <- matrix(unlist(lapply(logistic, function(x) lapply(x, function(x) apply(x, 2, function(x) quantile(x, probs = 0.75, na.rm = T))[2]))),
#                     nrow = length(or.z.vals), byrow = FALSE)[, -c(3, 5, 8, 9)]
# colnames(medians) <- colnames(lowers) <- colnames(uppers) <- colnames(logistic.med.table)[-1]
# 
# setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/Document")
# pdf(file = "logistic_mediqr_figure.pdf", height = 4, width = 10.5)
# par(mfrow = c(1, 3))
# par(mar = c(5.1, 5.1, 4.1, 2.1))
# 
# # Approximate ML
# plot(1: 7, medians[, "Approx. ML"], xlim = c(0.5, 7.5), ylim = c(-1, 0.75), pch = 16, cex = 0.8, cex.main = 1.6, cex.lab = 1.3, cex.axis = 1.15,
#      main = expression(paste("ML estimates of ", beta[X])),
#      ylab = "Median (IQR)", xlab = expression(OR[ZY]), xaxt = "n")
# axis(1, at = 1: 7, labels = sprintf("%.2f", or.z.vals), cex.axis = 1.15) 
# for (ii in 1: 7) {
#   lines(x = rep(ii, 2), y = c(lowers[ii, "Approx. ML"], uppers[ii, "Approx. ML"]))
#   lines(x = c(ii - 0.05, ii + 0.05), y = rep(lowers[ii, "Approx. ML"], 2))
#   lines(x = c(ii - 0.05, ii + 0.05), y = rep(uppers[ii, "Approx. ML"], 2))
# }
# abline(h = -0.33, lty = 2)
# 
# # RC
# plot(1: 7, medians[, "RC"], xlim = c(0.5, 7.5), ylim = c(-1, 0.75), pch = 16, cex = 0.8, cex.main = 1.6, cex.lab = 1.3, cex.axis = 1.15,
#      main = expression(paste("RC estimates of ", beta[X])),
#      ylab = "Median (IQR)", xlab = expression(OR[ZY]), xaxt = "n")
# axis(1, at = 1: 7, labels = sprintf("%.2f", or.z.vals), cex.axis = 1.15) 
# for (ii in 1: 7) {
#   lines(x = rep(ii, 2), y = c(lowers[ii, "RC"], uppers[ii, "RC"]))
#     lines(x = c(ii - 0.05, ii + 0.05), y = rep(lowers[ii, "RC"], 2))
#   lines(x = c(ii - 0.05, ii + 0.05), y = rep(uppers[ii, "RC"], 2))
# }
# abline(h = -0.33, lty = 2)
# 
# # PSC
# plot(1: 7, medians[, "PSC (no D)"], xlim = c(0.5, 7.5), ylim = c(-1, 0.75), pch = 16, cex = 0.8, cex.main = 1.6, cex.lab = 1.3, cex.axis = 1.15,
#      main = expression(paste("PSC estimates of ", beta[X])),
#      ylab = "Median (IQR)", xlab = expression(OR[ZY]), xaxt = "n")
# axis(1, at = 1: 7, labels = sprintf("%.2f", or.z.vals), cex.axis = 1.15) 
# for (ii in 1: 7) {
#   lines(x = rep(ii, 2), y = c(lowers[ii, "PSC (no D)"], uppers[ii, "PSC (no D)"]))
#     lines(x = c(ii - 0.05, ii + 0.05), y = rep(lowers[ii, "PSC (no D)"], 2))
#   lines(x = c(ii - 0.05, ii + 0.05), y = rep(uppers[ii, "PSC (no D)"], 2))
# }
# abline(h = -0.33, lty = 2)
# dev.off()
@

ML was median unbiased for all scenarios; RC was median unbiased for $0.91 \le OR_{ZY} \le 1.10$; and PSC was generally median biased (Figure \ref{logistic_mediqr_figure}).

%\begin{landscape}
\begin{figure}[h]
\centering
\includegraphics[height = 2.6in]{logistic_mediqr_figure.pdf}
\caption{Median (IQR) for various estimators in logistic regression simulations.}
\label{logistic_mediqr_figure}
\end{figure}

%\end{landscape}

Odds ratios outside $[0.91, 1.10]$ were included mainly to assess whether ML would outperform RC as the ``small measurement error'' assumption was stretched, or $\beta_Z^2 \sigma_\delta^2$ became large. It seems unlikely that a 100-kcal difference in daily caloric intake would be associated with more than a 10\% difference in odds of becoming pregnant. 




%The external validation study is the BioCycle Study, with details on the [website](https://www.nichd.nih.gov/about/org/diphr/eb/research/pages/biocycle.aspx). It is a longitudinal study of 259 women age 18-44 aimed at ``better understand[ing] the intricate relationship between reproductive hormone levels and oxidative stress during the menstrual cycle.'' Women were followed for two menstrual cycles. The women in this study are different from the women in EAGeR in that they did not necessarily have any prior miscarriages. 

\clearpage

\section{Discussion}

As data analysts increasingly leverage data from existing sources to explore new research questions, the problem of unmeasured confounding is common. When a potentially important confounder is missing, we propose seeking validation data and using popular methods from the measurement error literature to restore validity. In our simulations, RC and ML typically outperformed PSC, which was developed for unmeasured confounding and also requires validation data.

With external validation data, all of the methods under consideration require some special conditions to restore validity. For the regression calibration procedure to work, there has to be at least one variable in the measurement error model that is not in the disease model, i.e. is assumed independent of the outcome given the control variables. ML estimation is similarly dependent on one or more surrogates in the MEM, at least in the linear regression case. PSC relies on the assumption that the imperfect propensity score is independent of the outcome given the gold standard proensity score. This may be difficult to assess and, when violated, PSC can increase rather than decrease bias \citep{sturmer2005adjusting}.

There are some compelling aspects of PSC. First, it is simple to apply whether there is one confounder or several. Second, unlike ML and RC, it does not require identifying an additional variable available in both the main study and validation study that informs the confounder but is independent of the outcome conditional on disease model covariates. In seeking already collected validation data, it is challenging enough to find a study on a similar population with data on the confounder of interest and all covariates, without having to also identify a surrogate. %In the EAGeR/BioCycle example, a fatty acid that was moderately correlated with the unmeasured confounder, caloric intake, served that purpose well. But in general, requiring a surrogate makes it even more unlikely to find sufficient validation data. 

%if it is difficult to find a data source with the unmeasured confounder and all disease model covariates, in a similar population, it is even more difficult to additionally identify a surrogate that is measured in both studies. ... For ML and RC, one has to additionally identify a variable in both studies that informs the unmeasured confounder but not the outcome variable. 

Unfortunately, PSC gave biased estimates of the adjusted exposure effect in almost all external validation scenarios considered. Performance was particularly poor in the logistic regression simulations, where the average estimated odds ratio drifted from 1.86 to 0.45 as the unmeasured confounder's effect on the outcome variable was varied. Indeed, \citet{sturmer2007performance} found that PSC tended to overadjust even when the surrogacy assumption is met.

Other disadvantages of PSC are that it only works with a binary exposure, and it only produces an effect estimate for that exposure. In many cases, the investigator may wish to interpret adjusted associations for all of the covariates in the disease model, not just the exposure. In some instances there may not even be a particular covariate that is viewed as the primary exposure of interest.

Returning to the dependency of ML and RC on the availability of one or more surrogates, note that an entirely different variable may not be necessary. One could exploit non-linearity or interaction in the measurement error model, with higher-order or interaction term serving as surrogates. For example, in the EAGeR/BioCycle example, the disease model included the unmeasured confounder, caloric intake, as well as low Vitamin D, age, and obesity status. If the effect of age on caloric intake was quadratic, or if there was an age-by-obesity ineraction on caloric intake, one could use ML or RC even without the fatty acid level that was used as a surrogate in the example.

Another case where a surrogate may not be necessary for ML and RC to restore validity in the external validation case is when the unmeasured confounder is binary. In Scenario 4, the covariate $C$ was standard normal and $Z|C$ and $X|(Z, C)$ were logistic regression models. RC is rarely used for binary variables (one example is \citet{cole2006multiple}), but we implemented the usual procedure with logistic regression for $Z|(X, C)$. In contrast to the linear case, the estimated probabilities for $Z$ are not linear in $X$ and $C$, and thus the design matrix for fitting the disease model has full rank (unless $X$ is binary and there is no $C$, or both are binary and there is no interaction term). In the same context, ML does not have the identifiability problem that it has when a linear model is used for $Z|(X, C)$ is a linear model. However, the logistic function is relatively linear away from the tails, which translates to high collinearity among $X$, $C$, and $Z$, and thus relatively inefficient estimation of the regression parameters. We did observe some upward bias in ML when the external validation sample size was small.

It was interesting that in Scenario 4 ML estimation could produce valid estimates of the exposure effect even without any validation data. This situation has been discussed by other authors \citep{carroll2006measurement, fullermeasurement}. Relatively large sample sizes were needed to get reasonably efficient estimates in our simulation scenario, around 5,000, but such sample sizes are typical in studies that might serve as the main dataset (e.g. population-based studies, large cohort studies, disease registries). In the simulations, ML was not terribly inefficient with main study data only (SD = 0.124 with $n_E = 0$, SD = 0.077 with $n_E = 100). 

It is unclear whether a main study only approach could be useful in practical settings. With no data at all on the binary confounder $Z$, an analyst could not be confident that the ML procedure successfully targeted the confounder of interestor rather than some other binary characteristic. Most likely the algorithm would pick up whatever binary characteristic is most strongly associated with the outcome variable. Then, somewhat counterintuitively, perhaps the greater the magnitude of confounding due to a binary covariate, or at least the stronger the association between the covariate and outcome variable, the more feasible a main study only approach becomes. If the binary unmeasured confounder is known to be a strong risk factor for the disease, perhaps the strongest known risk factor, the 2-model ML correction might work well. Such a scenario seems fairly plausible; if an analyst is considering corrective methods to account for an unmeasured confounder, chances are that variable is established predictor of the outcome variable. This warrants future study. %and it may be reasonable to assume that the binary unmeasured confounder does has a stronger effect on the outcome than any other binary characteristic, for example smoking status in a lung cancer disease registry. 

Across all scenarios, internal validation data was drastically more valuable than external validation data. A practical advantage of having internal validation data is that one does not have to make an unverifiable transportability assumption. Transportability means assuming that two groups of people recruited for entirely different studies are similar enough that the relationship between the unmeasured confounder and the measurement error model covariates are the same in the two studies. In the logistic regression data example, the EAGeR and BioCycle both consisted of women of menstruating age, but those in EAGeR all had one or two prior miscarriages, were trying to get pregnant, and had different demographics than the women in BioCycle. The measurement error model was caloric intake vs. a fatty acid, age, and obesity status; it is hard to say whether the relationships among these variables are likely the same in EAGeR as in BioCycle.

There are also clear validity and efficiency advantages associated with having internal rather than external validation data. ML, RC, and PSC all produced unbiased parameter estimates in every simulation scenario when validation data were internal, yet all three were biased in at least one external validation scenario. In cases where ML and RC were valid with either type of validation data, a fixed internal validation sample size always produced much more efficient estimates than the same external validation sample size. With external validation data, one cannot improve upon the efficiency of the hypothetical case where the confounder was available in the main study, regardless of the validation sample size. With internal validation data, the corrected estimator can be more efficient than the hypothetical complete data estimator, and in fact always is more efficient when the validation sample size is at least as large as the main study sample size.

While the regression calibration procedure is straightforward when validation data are external and there is a surrogate, several variations are possible when validation data are internal. When there is a surrogate, the weighted procedure described by \citet{spiegelman2001efficient} is preferred. \citet{cole2006multiple} instead used the algebraic view of RC, defining the estimator as the estimated coefficient for the surrogate in the naive disease model fit (presumably using all available data, i.e. both main study and validation study observations) divided by the estimated coefficient for the surrogate in the measurement error model. This is equivalent to the conditional expectation procedure, where imputed values are used even where the true values are actually observed. In simulations not shown for Scenario 2, we found that this approach was less efficient than the conditional expectation procedure where true values are used where observed, which was in turn less efficient than the weighted method. Notably, the two suboptimal RC procedures gained virtually no efficiency by adding main study observations for a fixed internal validation sample size, which is a very undesirable property. Thus if one has internal validation data and a surrogate and wishes to use RC, we strongly recommend using the weighted procedure. But with internal validation data perhaps a better solution would be to relax the surrogacy assumption and use ML.

With internal validation data and no surrogate (Scenarios 1 and 3), we are not aware of a version of RC that rivals ML in terms of efficiency, particularly for large main study sample sizes. Therefore we currently recommend ML for this situation. Future work on developing an efficient version of RC for this scenario is warranted.

Propensity score calibration performed rather poorly in external validation scenarios, but we found that it performed surprisingly well with internal validation data. Relaxing the surrogacy assumption resulted in valid estimation of the exposure effect in all scenarios, with efficiency generally similar to RC.

We conclude with a few practical points on handling an unmeasured confounding problem. First, if an investigator is collecting data and realizes that a potential confounder has not been included, an optimal solution is to simply start measuring that variable. Even if the study is 90\% complete, measuring the potential confounder on the remaining 10\% will produce a very useful main study/internal validation study dataset. Analyzing the validation sample may reveal that the variable is not a confounder and can be omitted entirely from the analysis. If not, the investigator can implement ML, PSC, or RC. If the investigator is unable to measure the confounder in a subset, he or she will have to search for an external validation dataset. Even if one is located, the validity of the resulting estimate will depend on an unverifiable transportability assumption, and will likely be much less precise than if an internal sample was available.

Similarly, if one is planning to collect new validation data to combine with a larger main dataset, we strongly recommend measuring the outcome variable where practical. The feasibility of this likely depends on the type of study. If the main data is from a cross-sectional or retrospective cohort study, one could conceivably perform a much smaller study of the same type, measuring all the same variables plus the confounder unavailable in the main study; conducting a mini 10-year prospective cohort study, on the othe rhand, would be impractical. Again, the motivation for collecting outcome data is to produce an internal rather than external validation sample, resulting in much greater statistical efficiency.

Taking a slightly different view, suppose an investigator facing an unmeasured confounding situation decides to simply conduct a new study measuring every variable in the disease model. The dataset missing the unmeasured confounder can still be extremely useful. Instead of using only the new study to estimate the exposure effect, you view the new study as an internal validation sample, and the original study as a ``main '' dataset. Combining the two datasets and using one of the corrective methods described here can result in a potentially major efficiency gain compared to discarding the incomplete dataset.

In summary, maximum likelihood and regression calibration approaches originally developed for measurement error problems are well suited for the unmeasured confounding setting. In our simulations, ML and RC generally outperformed PSC. Internal validation data is always preferred over external validation, as it results in much more precise estimates that are valid in general without assuming that special conditions hold (surrogacy, transportability).


% Notes:
% 
% \begin{itemize}
%   \item Maybe use same scale for 3 parts of Figure 1.6.
%   \item In log reg, What if main study is case-control and internal validation study is cross-sectional... Or both case-control but with different sampling probabilities. How to accommodate different intercepts? For ML, just estimate both intercepts. For RC with cond. exp, need to think about. Greenland should work fine as is but need to think about that. Weighted combination of each slope, just throw intercept away, not of interest anyway.
% \end{itemize}



\mychapter{2}{Chapter 2: Regression with continuous pooled exposure subject to measurement error and processing error}

\section{Motivation for pooling}

In epidemiology, pooling refers to the practice of combining samples from multiple participants and measuring a biomarker of interest in the combined sample. This is in contrast to the usual approach of measuring each variable on each participant. If the assay measures the concentration or total amount of a continuous biomarker, a pooled measurement represents the mean or sum of the biomarker across all members of the pool; if it measures presence of absence of a biomarker, a pooled measurement indicates whether the biomarker is present in at least one member of the pool. 

Early applications of pooling were mostly aimed at determining individual level disease status with lower costs. For example, \citet{dorfman1943detection} reported on the use of pooling to detect cases of syphilis in the U.S. military. When pooled samples tested negative, all members of the pool were cleared; when pooled samples tested positive, each member was tested individually. Compared to individual testing, this study design reduces the number of tests required, as long as the disease prevalence is less than 30\%. The rarer the disease, the greater the efficiency gain. At 1\% prevalence, costs can be reduced by 80\%.

More recently, statisticians have considered the use of pooling in situations where some information loss does occur. For example, \citet{weinberg1999using} described a scenario where the cost of measuring the level of a continuous environmental exposure was over \$1,000. Pooling specimens could drastically reduce costs. Of course, if a budget only allowed for a limited number of assays, one could simply take a random sample of individual study participants on which to measure the exposure. An advantage to pooling is that it does not ``throw away'' any of the study data. More importantly, pooling designs often result in little loss in statistical power compared to having complete individual level data, especially if steps are taken to assign similar subjects to be in the same pools \citep{weinberg1999using, mitchell2014highly}.

It stands to reason that if one can reduce the number of expensive assays while sacrificing little statistical efficiency, maximizing power for a given budget may often favor a pooling design with more participants (but fewer total assays) than the usual design. Indeed, \citet{weinberg1999using} argue that pooling could be advantageous in many common scenarios. 

Figure \ref{pooling_twosamplet} illustrates potential efficiency gains for a two-sample t-test, where the true mean difference is 0.5, the standard deviation is 1, each assay costs \$500, and other costs to recruit each participant are \$100. Achieving 80\% power with individual testing requires 64 participants per group, at a total cost of \$76,800; with a 10-person pooled design, it requires 80 participants per group, with 16 assays and a total cost of \$24,000. For the same cost as the individual design with 80\% power, pooling designs with 2, 3, and 10 members per pool achieve powers of 0.954, 0.987, and $> 0.999$.

\vspace{0.1in}
<<eval = F, echo = F, results = "asis", fig.height = 4, fig.width = 4>>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
poolpower.twosamplet <- function(mu.diff, sigma, assay.cost, recruitment.cost, poolsizes = c(1, 2, 5),
                                 alpha = 0.05, both.tails = F, units = "$") {
  
  # Sort poolsizes vector and make sure first element is 1
  poolsizes <- sort(poolsizes)
  if (poolsizes[1] != 1) {
    poolsizes <- c(1, poolsizes)
  }
  
  # Check that there are 5 or fewer pool size values
  if (! length(poolsizes) %in% 2: 5) {
    stop("The poolsizes vector should include at least 2 and no more than 5 values.")
  }
  
  # Figure out number of individual measurements that would give power of 0.99
  n <- 2: 1000
  power <- power.t.test(n = n, delta = mu.diff, sd = sigma, sig.level = alpha, type = "two.sample", strict = both.tails)$power
  loc.max.n <- which(power > 0.99)[1]
  max.n <- n[loc.max.n]
  power <- power[1: loc.max.n]
  n <- n[1: loc.max.n]
  if (max.n %% 2 == 1) {
    max.n <- max.n + 1
  }
  
  # Create results data frame for individual level data and add it to growing list
  g1.df <- data.frame(n.pergroup = n, assays.pergroup = n, recruitment.costs = 2 * n * recruitment.cost, assay.costs = 2 * n * assay.cost, 
                      total.costs = 2 * n * (recruitment.cost + assay.cost), power = round(power, 3))
  ret.list <- list(g1 = g1.df)
  
  # Pool sizes ranging from 2 to max.poolsize
  list.index <- 1
  k <- 2: max.n
  for (ii in poolsizes[-1]) {
    pooled.power <- power.t.test(n = k, delta = mu.diff, sd = sigma / sqrt(ii), type = "two.sample", strict = both.tails)$power
    gii.df <- data.frame(n.pergroup = k * ii, assays.pergroup = k, recruitment.costs = 2 * k * ii * recruitment.cost, 
                         assay.costs = 2 * k * assay.cost, total.costs = 2 * k * ii * recruitment.cost + 2 * k * assay.cost, 
                         power = round(pooled.power, 3))
    list.index <- list.index + 1
    ret.list[[list.index]] <- gii.df
    
  }
  names(ret.list) <- paste("g", poolsizes, sep = "")
  
  # Create plot of power vs. total costs for various pool sizes
  plot(c(0, 1), c(0, 1), type = "n", main = "Power vs. Total Costs", cex.axis = 0.8,
       ylab = "Power", xlab = paste("Total costs, recruitment and assays (", units, ")", sep = ""),
       ylim = c(0, 1), yaxt = "n", xlim = c(0, max(ret.list[[1]][, "total.costs"])))
  axis(side = 2, at = c(0, 0.5, 0.8, 0.9, 1), cex.axis = 0.8)
  colors <- c("black", "purple", "blue", "red", "orange")[1: length(poolsizes)]
  for (ii in 1: length(ret.list)) {
    
    df <- ret.list[[ii]]
    points(x = df$total.costs, y = df$power, col = colors[ii], type = "l")
    points(x = df$total.costs, y = df$power, col = colors[ii], type = "p", pch = 1, cex = 0.5)
    
  }
  legend("bottomright", col = colors, pch = 1, pt.cex = 0.5, legend = paste("Pool size = ", poolsizes, sep = ""))
  abline(h = 0.8, lty = 2)
  abline(h = 0.9, lty = 2)
  abline(h = 1.0, lty = 2)
  
  # Return list of results
  return(ret.list)
  
}

pdf(file = "pooling_twosamplet.pdf", height = 5, width = 5.5)
par(mar=c(4.1,4.1,3.1,2.1))
abc <- poolpower.twosamplet(mu.diff = 0.5, sigma = 1, assay.cost = 0.5, recruitment.cost = 0.1, both.tails = T, 
                            units = "$1,000's", poolsizes = c(1, 2, 3, 10))
dev.off()
ind.data <- abc$g1
ind.data[which(ind.data$power > 0.8)[1], ]
pool10.data <- abc$g10

xvals <- rnorm(n = 10000)
yvals <- 0.25 + 1.25 * xvals + rnorm(n = 10000)
par(mfrow = c(1, 2))
plot(xvals, yvals)
plot(yvals, xvals)
summary(lm(yvals ~ xvals))
summary(lm(xvals ~ yvals))
@

\begin{figure}[h]
\centering
\includegraphics[height = 4.25in]{pooling_twosamplet.pdf}
\caption{Power vs. total costs for hypothetical two-sample t-test.}
\label{pooling_twosamplet}
\end{figure}

\clearpage

%There are still other benefits of pooling. When the volume of individual samples is insufficient for a particular assay, pooling multiple samples can generate the required volume; the volume of individual samples may be insufficient for testing, but the volume of several samples pooled 


\section{Analyzing data with continuous exposure subject to pooling} \label{Analyzing data with continuous exposure subject to pooling}

In the syphilis example, pooling was simply a means of determing individual level disease status while reducing costs. Pooling in this scenario results in no information loss, and in a research setting one could use standard statistical techniques using individual level disease status.

When measuring a continuous exposure in pools, the individual level data cannot be recovered. In analyzing this data, one might think to use the pooled measurement as each member's individual value, or use pooled data as the unit of analysis. The first approach is always invalid, as it essentially induces a measurement error scenario; with some adjustments depending on the scenario, the second approach is often valid.

\subsection*{Linear regression}

Consider linear regression, where interest is in estimating the association between a continuous exposure $X$ subject to pooling and a continuous outcome $Y$, controlling for a covariate vector $\bm{C}$. The individual level model for $Y$ vs. $(X, \ \bm{C})$ for the $j^{th}$ member of the $i^{th}$ pool is:
%
\begin{equation}
\label{linear_individual_model}
Y_{ij} = \beta_0 + \beta_x X_{ij} + \bm{\beta_c}^T \bm{C}_{ij} + \epsilon_{ij}, \ \epsilon_{ij} \stackrel{\text{iid}}{\sim} (0, \ \sigma^2)
\end{equation}
%
Within each pool, we observe the summed exposure for all members of the pool, $X_i$, rather than individual $X_{ij}$'s. $Y$ and $\bm{C}$ may be measured at the individual level, but we can calculate the poolwise sums:
%
\vspace{-0.2in}
\begin{equation}
Y_i^* = \sum_{j=1}^{g_i} Y_{ij} \qquad 
\bm{C}_i^* = \sum_{j=1}^{g_i} \bm{C}_{ij} \qquad
\epsilon_i^* = \sum_{j=1}^{g_i} \epsilon_{ij}
\end{equation}
%
This motivates the poolwise model relating $Y_i$ to $(X_i, \ \bm{C}_i)$:
%
\begin{equation}
\label{linear_poolwise_model}
Y_i^* = \sum_{j=1}^{g_i} Y_{ij} = g_i \beta_0 + \beta_x X_i^* + \bm{\beta_c}^T \bm{C}_i^* + \epsilon_i^*, \ \epsilon_i^* \stackrel{\text{ind}}{\sim} (0, \ g_i \sigma_\epsilon^2)
\end{equation}
%
While the regression coefficients for $X_i$ and $\bm{C}_i$ are the same as in the individual model, the intercept and error variances are multiplied by the number of members in each pool, $g_i$. If all pool sizes are the same, one can simply use OLS on the poolwise values. If the intercept and error variance are of interest, $\beta_0$ and $\sigma^2$ can be estimated by dividing the estimated poolwise intercept and poolwise MSE, respectively, by the common pool size $g$.

If pool sizes are different, efficient estimation of $\bm{\beta}$ requires weighted least squares (WLS), with weights equal to $\frac{1}{g_i}$ for each pooled observation. To accommodate the multiplied intercept, the model should be fit with $g_i$ as a covariate and no intercept; the regression coefficient for $g_i$ represents $\beta_0$. %\citep{lyles2015nant function estimatorinant, weinberg1999using}.

\subsection*{Logistic regression}

Consider the case where a continuous exposure is measured in random pools of like participants (i.e. each pool consists of all cases or all controls). Suppose an individual-level logistic regression model relates disease status $Y_{ij}$ to exposure $X_{ij}$ and covariates $\bm{C}_{ij}$:
%
\begin{equation}
\label{logistic_individual_model}
\text{logit}[P(Y_{ij} = 1)] = \beta_0 + \beta_x X_{ij} + \bm{\beta_c}^T \bm{C}_{ij}
\end{equation}
%
\citet{weinberg1999using} show that the corresponding poolwise model is:
%
\vspace{-0.25in}
%
\begin{equation}
\label{logistic_poolwise_model}
\text{logit}[P(Y_i = 1)] = g_i \beta_0 + \beta_x X_i^* + \bm{\beta_c}^T \bm{C}_i^* + q_i
\end{equation}
%
where $Y_i = 1$ if all members of the pool are cases and 0 if all are controls, $X_i^*$ and $\bm{C}_i^*$ are the summed exposure and covariate values across all members of the pool, respectively, and $q_i$ is an offset. Analyzing poolwise data in this scenario therefore requires fitting a logistic regression model with the pool size as a covariate, poolwise sums for $X_i^*$ and $\bm{C}_i^*$, no intercept, and an offset of $q_i$.

The offset $q_i$ is given by \citet{weinberg2014correction} as:
%
\vspace{-0.1in}
\begin{equation}
\label{wu_offset}
q_i = g_i \ \text{ln}(\frac{P(A|D)}{P(A|\bar{D})}) + g_i \ \text{ln}(\frac{n_0}{n_1}) + \text{ln}(\frac{\text{\# case pools of size } g_i}{\text{\# control pools of size } g_i})
\end{equation}
%
where $P(A|D)$ and $P(A|\bar{D})$ are accrual probabilities for cases and controls and $n_1$ and $n_0$ are the total number of case and control observations (not pools) in the study. 

We note that $P(A|D) = \frac{n_1}{N_1}$, the number of case observations sampled divided by the number of cases in the entire population, and likewise $P(A|\bar{D}) = \frac{n_0}{N_0}$. Thus the first term can be expressed as $g_i \ \text{ln}(\frac{n_1/N_1}{n_0/N_0}) = g_i \ \text{ln}(\frac{n_1}{n_0} \frac{N_0}{N_1}) = g_i \ \text{ln}(\frac{n_1}{n_0}) + g_i \ \text{ln}(\frac{N_0}{N_1})$. The first part here cancels out $g_i \ \text{ln}(\frac{n_0}{n_1})$ in \ref{wu_offset}, and the second part is equivalent to $g_i \ \text{ln}(\frac{1-p}{p})$, where $p$ is the disease prevalence. So the offset can also be written:
%
\vspace{-0.1in}
\begin{equation}
\label{wu_offset2}
q_i = g_i \ \text{ln}(\frac{1-p}{p}) + \text{ln}(\frac{\text{\# case pools of size } g_i}{\text{\# control pools of size } g_i})
\end{equation}
%
Valid estimation of $(\beta_0, \ \beta_x, \ \bm{\beta_c}^T)$ is possible with poolwise data whether prospective sampling or case-control sampling is utilized \citep{lyles2016efficient}. Valid estimation of $\beta_0$ requires prospective sampling (in which case the first term in \ref{wu_offset} is 0) or case-control sampling with known case and control accrual probabilities (to insert into the first term in \ref{wu_offset}) or known disease prevalence (to insert into the first term in \ref{wu_offset2}).


\section{Accounting for measurement error and processing error}

Lab assays performed on pooled samples are prone to two types of error: (1) Measurement error due to imprecision in the assay itself, which would be present whether specimens were from single or multiple individuals; and (2) Processing error that arises specifically as a result of combining multiple samples. Potential sources of processing error include cross-reactions among chemical agents from different specimens \citep{weinberg1999using} and unintentionally unequal alliquot volumes across members of pools. 

Corrective methods for covariate measurement error in individual-level analyses typically require reliability or validation data. One could take a similar approach with pooling. Validation data requires a gold standard test that may not be available, but replicate measurements of individual specimens and pooled specimens could be used to estimate the measurement error variance and the sum of the measurement error variance and processing error variance, respectively.

But our main focus is on leveraging the pooling study design itself to perform measurement error corrections, without requiring any validation or reliability data. Building on previous work by \citet{schisterman2010hybrid} and \citet{lyles2015discriminant}, suppose that instead of the true pooled sum $X_i^*$ we observe an imprecise version $\widetilde{X}_i^*$ subject to both measurement error and processing error:
%
\begin{equation}
\widetilde{X_i}^* = X_i^* + \epsilon_i^m + \epsilon_i^p I(g_i > 1)
\end{equation}
%
We are considering pooling designs with several different pool sizes, perhaps including some individual measurements where $g_i = 1$. The processing error only applies to pooled measurements, hence the indicator function attached to $\epsilon_i^p$.

For a maximum likelihood approach, the likelihood contribution for pool $i$ given the observed $(Y_i, \ \widetilde{X}_i^*, \ \bm{C}_i^*)$ is:
%
\begin{equation}
L_i = f(y_i^*, \widetilde{x}_i^*, \bm{c}_i^*) \propto f(y_i^*, \widetilde{x}_i^* | \bm{c}_i^*)
\end{equation}
%
When there are covariates $\bm{C}$, we need to specify a model for $X_{ij}|\bm{C}_{ij}$. One might prefer specifying a poolwise model, since data on hand to inform the model would be poolwise. But pools of different size have different error variances, so we must specify an individual level model. Including some individual measurements could be useful for specifying this model. If a linear model is reasonable:
%
\begin{equation}
\label{xc_individual}
X_{ij} = \alpha_0 + \bm{\alpha_C}^T \bm{C}_{ij} + \epsilon_{ij}^x, \ \epsilon_{ij}^x \stackrel{\text{iid}}{\sim} (0, \ \sigma_{x|c}^2)
\end{equation}
%
The corresponding poolwise model is:
%
\begin{equation}
\label{xc_poolwise}
X_i^* = g_i \alpha_0 + \bm{\alpha_C}^T \bm{C}_i^* + \epsilon_i^{x*}, \ \epsilon_i^{x*} \stackrel{\text{ind}}{\sim} (0, \ g_i \sigma_{x|c}^2)
\end{equation}
%

\subsection*{Linear regression}

Using the poolwise linear regression model \ref{linear_poolwise_model}, we can work towards obtaining a closed-form expression for the likelihood by writing:
%
\renewcommand{\arraystretch}{1}
\begin{equation}
\left[ \begin{array}{c} Y_i^* \\ 
                        \widetilde{X}_i^* \end{array} \right] = 
\left[ \begin{array}{c} g_i \beta_0 + \beta_x X_i^* + \bm{\beta_c}^T \bm{C}_i^* + \epsilon_i^* \\ 
                        X_i^* + \epsilon_i^p I(g_i > 1) + \epsilon_i^m \end{array} \right]
\end{equation}
%
Replacing $X_i^*$ with the poolwise model for $X_i^*|\bm{C}_i^*$ from \ref{xc_poolwise}, and rearranging:
%
\begin{equation}
\label{bivariate_normality1}
\left[ \begin{array}{c} Y_i^* \\ 
                        \widetilde{X}_i^* \end{array} \right] = 
\left[ \begin{array}{c} g_i (\beta_0 + \beta_x \alpha_0) + (\beta_X \bm{\alpha_c}^T + \bm{\beta_c}^T) \bm{C}_i^* \\
                        g_i \alpha_0 + \bm{\alpha_c}^T \bm{C}_i^* \end{array} \right] + 
\left[ \begin{array}{cccc} 1 & \beta_X & 0 & 0 \\ 
                           0 & 1 & I(g_i > 1) & 1 \end{array} \right]
\left[ \begin{array}{c} \epsilon_i^* \\ \epsilon_i^{x*} \\ \epsilon_i^p \\ \epsilon_i^m \end{array} \right]
\end{equation}
%
If we assume that the errors terms are normal and independent, $\bm{\epsilon}_i = (\epsilon_i^*, \ \epsilon_i^{x*}, \ \epsilon_i^p, \ \epsilon_i^m)^T \sim N_4(\bm{0}_4, \ \bm{\Sigma}_i)$ where $\bm{\Sigma}_i$ is a diagonal matrix with $(g_i \sigma^2, \ g_i \sigma_x^2, \ \sigma_p^2, \ \sigma_m^2))$ on the diagonal. From normal theory it follows that: 
%$(Y_i, \ X_i^*)|\bm{C}_i \sim N_2(\mu_i, \ \bm{A} \bm{\Sigma}_i \bm{A}^T)$ with $A = 3$.
%We assume that the errors terms are normal and independent, or $\bm{\epsilon}_i = (\epsilon_i, \epsilon_i^x, \epsilon_i^m, \epsilon_i^p)^T \sim N_4(\bm{0}_4, D(g_i \sigma^2, g_i \sigma_x^2, \sigma_m^2, \sigma_p^2))$ where $D(\bm{x})$ is defined as a square matrix with $\bm{x}$ on the diagonal. From normal theory it follows that:
%
\begin{equation}
\label{bivariate_normality2}
\left[ \begin{array}{c} Y_i^* \\ 
                        \widetilde{X}_i^* \end{array} \right] \ | \ \bm{C}_i^* \sim N_2 \left(
\left[ \begin{array}{c} g_i (\beta_0 + \beta_x \alpha_0) + (\beta_x \bm{\alpha_c}^T + \bm{\beta_c}^T) \bm{C}_i^* \\
                        g_i \alpha_0 + \bm{\alpha_c}^T \bm{C}_i^* \end{array} \right], \ 
\left[ \begin{array}{cc} g_i (\sigma^2 + \beta_x^2 \sigma_x^2) & g_i \beta_x \sigma_x^2 \\
                         g_i \beta_x \sigma_x^2 & g_i \sigma_x^2 + \sigma_p^2 I(g_i > 1) + \sigma_m^2 \end{array} \right] \right)
\end{equation}
%
Thus the likelihood is a bivariate density with mean and variance as indicated. As long as there are at least three different pool sizes, the variance components are all identifiable and ML can be expected to produce valid estimates of the regression parameters in model \ref{linear_individual_model}.

\subsection*{Logistic regression: ML}

Moving to the poolwise logistic regression model \ref{logistic_poolwise_model}, a convenient way to factor the likelihood for a full or approximate ML analysis is:
%
\begin{equation}
\begin{aligned}
L_i & = f(y_i^*, \widetilde{x}_i^* | \bm{c}_i^*) = \int_{x_i^*} f(y_i^*|x_i^*, \widetilde{x}_i^*, \bm{c}_i^*) \cdot f(x_i^*|\widetilde{x}_i^*, \bm{c}_i^*) \cdot f(\widetilde{x}_i^* | \bm{c}_i^*) \ dx_i^* \\
 & = f(\widetilde{x}_i^* | \bm{c}_i^*) \cdot \int_{x_i^*} f(y_i^*|x_i^*, \bm{c}_i^*) \cdot f(x_i^*|\widetilde{x}_i^*, \bm{c}_i^*) \ dx_i^*
\end{aligned}
\end{equation}
%
The three densities are determined by the distributions:
%
\begin{small}
\begin{equation}
\begin{aligned}
& \widetilde{X}_i^*|\bm{C}_i^* \sim N(g_i \alpha_0 + \bm{\alpha_c}^T \bm{C}_i^*, \ g_i \sigma_x^2 + \sigma_p^2 I(g_i > 1) + \sigma_m^2) \\
& Y_i^*|(X_i^*, \bm{C}_i^*) \sim \text{Bern}[(1 + e^{-g_i \beta_0 - \beta_x X_i^* - \bm{\beta_c}^T C_i^* - q_i})^{-1}] \\
& X_i^*|(\widetilde{X}_i^*, \bm{C}_i^*) \sim N(\mu = g_i \alpha_0 + \bm{\alpha_c}^T \bm{C}_i^* + \frac{g_i \sigma_x^2}{g_i \sigma_x^2 + \sigma_p^2 I(g_i > 1) + \sigma_m^2}(\widetilde{X}_i^* - g_i \alpha_0 - \bm{\alpha_c}^T \bm{C}_i^*), \\
& \hspace{1.08in} \sigma^2 = g_i \sigma_x^2 - \frac{g_i^2 \sigma_x^4}{g_i \sigma_x^2 + \sigma_p^2 I(g_i > 1) + \sigma_m^2})
\end{aligned}
\end{equation}
\end{tiny}

% \vspace{0.1in}
% \begin{itemize}
% \item[] $X_i^*|\bm{C}_i \sim N(g_i \alpha_0 + \bm{\alpha_C}^T \bm{C}_i, \ g_i \sigma_x^2 + \sigma_m^2 + \sigma_p^2 I(g_i > 1))$
% \item[] $Y_i|(X_i, \bm{C}_i) \sim \text{Bern}[(1 + e^{-g_i \beta_0 - \beta_X X_i - \bm{\beta_C}^T C_i - \text{ln}(r_{g_i})})^{-1}]$
% \item[] $X_i|(X_i^*, \bm{C}_i) \sim N \left( g_i \alpha_0 + \bm{\alpha_C}^T \bm{C}_i + \frac{g_i \sigma_x^2}{g_i \sigma_x^2 + \sigma_m^2 + \sigma_p^2 I(g_i > 1)}(X_i^* - g_i \alpha_0 - g_i \bm{\alpha_C}^T \bm{C}_i), \ g_i \sigma_x^2 - \frac{g_i^2 \sigma_x^4}{g_i \sigma_x^2 + \sigma_m^2 + \sigma_p^2 I(g_i > 1)} \right)$
% \end{itemize}
% \vspace{0.1in}

The first two are straightforward, and the third can be derived using a similar approach as in \ref{bivariate_normality1} and \ref{bivariate_normality2}, and then using the conditional distribution property for multivariate normal random variables. %The joint distribution of $(X_i, X_i^*)|\bm{C}_i$ is:
%
% \begin{equation}
% \left[ \begin{array}{c} X_i \\ X_i^* \end{array} \right] \ | \ \bm{C}_i \sim N_2 \left(
% \left[ \begin{array}{c} g_i \alpha_0 + \bm{\alpha_C}^T \bm{C}_i \\
%                         g_i \alpha_0 + \bm{\alpha_C}^T \bm{C}_i \end{array} \right], \ 
% \left[ \begin{array}{cc} g_i \sigma_x^2 & g_i \sigma_x^2 \\
%                          g_i \sigma_x^2 & g_i \sigma_x^2 + \sigma_m^2 + \sigma_p^2 I(g_i > 1) \end{array} \right] \right)
% \end{equation}
% %
% Thus $X_i|(X_i^*, \bm{C}_i)$ is normally distributed with mean and variance as given.

With the likelihood fully specified, optimization routines can be used to maximize the likelihood and estimate the regression parameters of interest. If one wishes to perform a full ML analysis, the unobserved $x_i^*$'s have to numerically integrated out of the likelihood function for each pool. An attractive alternative is approximate ML, using a probit approximation to the logistic-normal integral $\int_{x_i^*} f(y_i^*|x_i^*, \bm{c}_i^*) \cdot f(x_i^*|\widetilde{x}_i^*, \bm{c}_i^*) \ dx_i^*$ as described in Chapter 1.


\subsection*{Logistic regression: Discriminant function approach}

An alternative to ML in this setting was recently developed by \citet{lyles2015discriminant}. The method utilizes the fact that a log-odds ratio for a continuous predictor $X$ in a logistic regression model can be estimated by fitting a linear regression of $X$ vs. the binary outcome $Y$ and any covariates $\bm{C}$, assuming that $X|(Y, \bm{C})$ is a linear regression model with homoskedastic, normal errors \citep{cornfield1962joint}.

% Suppose that $Y$ is binary and $X|(Y, \bm{C})$ \sim \text{N}(\gamma_0 + \gamma_Y Y + \bm{\gamma_C}^T \bm{C}, \ \sigma_d^2)$. Using Bayes rule:
% %
% \begin{equation}
% %\begin{aligned}
% P(Y = 1|X, \bm{C}) = \frac{f(X|\bm{C}, Y = 1) \cdot P(Y = 1|\bm{C}) \cdot f(\bm{C})}{f(X|\bm{C}) \cdot f(\bm{C})} = \frac{f(X|\bm{C}, Y = 1) \cdot P(Y = 1 | \bm{C})}{f(X|\bm{C})}
% %\end{aligned}
% \end{equation}
% %
% Inserting the specified densities and simplifying:
% %
% \begin{equation}
% \begin{aligned}
% P(Y = 1|X, \bm{C}) & = \frac{\frac{1}{\sqrt{2 \pi \sigma_d^2}} e^{-\frac{1}{2 \sigma_d^2}(X-\gamma_0 - \gamma_Y - \bm{\gamma_C}^T \bm{C})^2} \ p}{\frac{1}{\sqrt{2 \pi \sigma_d^2}} e^{-\frac{1}{2 \sigma_d^2}(X-\gamma_0 - \gamma_Y - \bm{\gamma_C}^T \bm{C})^2} \ p + \frac{1}{\sqrt{2 \pi \sigma_d^2}} e^{-\frac{1}{2 \sigma_d^2}(X- \gamma_0 - \bm{\gamma_C}^T \bm{C})^2} \ (1 - p)} \\
%  & = \frac{1}{1 + e^{-\text{log}(\frac{p}{1 - p}) + \frac{2 \gamma_0 \gamma_Y + \gamma_Y^2}{2 \sigma_d^2} + \frac{\sigma_Y}{\sigma_d^2} X}}
% \end{aligned}
% \end{equation}
% %
% Thus $Y|X \sim \text{Bern}([1 + e^{-\beta_0^* - \beta_X^* X}]^{-1})$, where $\beta_0^* = \text{log}(\frac{p}{1 - p}) - \frac{2 \gamma_0 \gamma_Y + \gamma_Y^2}{2 \sigma_d^2}$ and $\beta_X^* = \frac{\gamma_Y}{\sigma_d^2}$.

Suppose that $Y \sim \text{Bern}(p)$ and $X|Y \sim \text{N}(\gamma_0 + \gamma_y Y, \ \sigma_d^2)$. Using Bayes rule:
%
\begin{equation}
%\begin{aligned}
P(Y = 1|X) = \frac{f(X|Y = 1) P(Y = 1)}{f(X)} = \frac{f(X|Y = 1) P(Y = 1)}{f(X|Y = 1) P(Y = 1) + f(X|Y = 0) P(Y = 0)}
%\end{aligned}
\end{equation}
%
Inserting the specified densities and simplifying:
%
\begin{equation}
\begin{aligned}
P(Y = 1|X) & = \frac{\frac{1}{\sqrt{2 \pi \sigma_d^2}} e^{-\frac{1}{2 \sigma_d^2}(X - \gamma_0 - \gamma_y)^2} \ p}{\frac{1}{\sqrt{2 \pi \sigma_d^2}} e^{-\frac{1}{2 \sigma_d^2}(X - \gamma_0 - \gamma_y)^2} \ p + \frac{1}{\sqrt{2 \pi \sigma_d^2}} e^{-\frac{1}{2 \sigma_d^2}(X - \gamma_0)^2} \ (1 - p)} \\
 & = \frac{1}{1 + e^{-\text{log}(\frac{p}{1 - p}) + \frac{2 \gamma_0 \gamma_y + \gamma_y^2}{2 \sigma_d^2} + \frac{\sigma_y}{\sigma_d^2} X}}
\end{aligned}
\end{equation}
%
Thus $Y|X \sim \text{Bern}([1 + e^{-\beta_0^* - \beta_x^* X}]^{-1})$, where $\beta_0^* = \text{log}(\frac{p}{1 - p}) - \frac{2 \gamma_0 \gamma_y + \gamma_y^2}{2 \sigma_d^2}$ and $\beta_x^* = \frac{\gamma_y}{\sigma_d^2}$. In other words, if a linear regression fit of $X$ vs. $Y$ is supported by the data, one can use the quantity $\frac{\hat{\gamma}_y}{MSE}$ to estimate the log-odds ratio for a logistic regression of $Y$ vs. $X$.

With covariates $\bm{C}$, a linear regression model with homoskedastic, normal errors for $X|(Y, \bm{C})$ implies a logistic regression model for $Y$ vs. $(X, \ \bm{C})$, as long as $\text{logit}[P(Y = 1|\bm{C})]$ is linear in $\bm{C}$ \citet{lyles2009fresh}.

While scarcely used in epidemiology, the discriminant function approach has some notable advantages over logistic regression in terms of bias, efficiency, and convergence \citet{lyles2009fresh}. Additionally, it generalizes nicely to the pooling scenario.

% For the individual logistic regression model with no measurement error or processing error, model \ref{logistic_individual_model}, suppose we flip $X_{ij}$ and $Y_{ij}$ and fit:
% %
% \begin{equation}
% X_{ij} = \gamma_0 + \gamma_Y Y_{ij} + \bm{\gamma_C}^T \bm{C}_{ij} + \epsilon_{ij}^D, \ \epsilon_{ij}^D \stackrel{\text{iid}}{\sim} N(0, \ \sigma_d^2)
% \end{equation}
% %
% Then the ratio $\frac{\gamma_Y}{\sigma_d^2}$ represents the same log-odds ratio as $\beta_X$ in model \ref{logistic_individual_model}. Thus a discriminant function log-odds estimator can be defined as:
% %
% \begin{equation}
% \label{betaxd}
% \hat{\beta}_{X, D} = \frac{\hat{\gamma}_Y^*}{\text{MSE}}
% \end{equation}
% %
% \citet{lyles2015discriminant} also define a uniformly minimum variance version of the estimator, which multiplies $\hat{\beta}_{X, D}$ by the factor $\frac{n-p-4}{n-p-2}$ where $p$ is the dimension of $\bm{C}$. Delta method variance estimators can be obtained for either version using linear regression theory.

To estimate the same covariate-adjusted log-odds ratio represented by $\beta_x$ in \ref{logistic_poolwise_model}, we can specify the poolwise discriminant function model:
%
\begin{equation}
X_i^* = g_i \gamma_0 + \gamma_y Y_i^* + \bm{\gamma_C}^T \bm{C}_i^* + \epsilon_i^{d*}, \ \epsilon_i^{d*} \stackrel{\text{ind}}{\sim} N(0, \ g_i \sigma_d^2)
\end{equation}
%
Note that $Y_i^*$ here is the number of cases in the $i^{th}$ pool, in contrast to $Y_i$ in \ref{logistic_poolwise_model} which was 1 if all pool members were cases and 0 if all were controls. This reflects an advantage of the discriminant function approach: it does not require that each pool consists of all cases or all controls.

Fitting this poolwise model (as described in Section \ref{Analyzing data with continuous exposure subject to pooling}, page \pageref{Analyzing data with continuous exposure subject to pooling}) produces $(\hat{\gamma}_y, \ \text{MSE})$, from which one can calculate $\widehat{\text{log-OR}}_{xy|\bm{c}} = \frac{\hat{\gamma}_y}{MSE}$.

When $X_i^*$ is subject to additive processing error and measurement error, $\widetilde{X}_i^*$ can be written as:
%
\begin{equation}
\begin{aligned}
\widetilde{X}_i^* & = X_i^* + \epsilon_i^p I(g_i > 0) + \epsilon_i^m \\
& = g_i \gamma_0 + \gamma_y Y_i^* + \bm{\gamma_c}^T \bm{C}_i^* + \epsilon_i^{d*} + \epsilon_i^p I(g_i > 0) + \epsilon_i^m
\end{aligned}
\end{equation}
%
The main difference between this model and previous poolwise linear regression models is that the WLS weights are not simply $\frac{1}{g_i}$. The error variances also depend on $(\sigma_d^2, \ \sigma_p^2, \ \sigma_m^2)$. We follow \citet{lyles2015discriminant} in using ML to estimate the parameters rather than iteratively reweighted least squares. With ML estimates of $\gamma_Y$ and $\sigma_d^2$, one can estimate the log-odds ratio corrected for processing error and measurement error.

The discriminant function approach has several advantages compared to logistic regression. It does not require like pools with respect to case status, it does not require specifying $f(x_{ij}|\bm{c}_{ij})$, and it has a closed-form likelihood; however, it requires conditional normality of $X_{ij}|(Y_{ij}, \bm{C}_{ij})$.


\subsection*{Incorporating special data types}

Each of these three poolwise methods can be adapted to accommodate study designs with the following special data types:

\begin{itemize}
  \item Replicate $\bm{X_i}^*$ measurements.
  \item Gold standard $X_i$ measurements. 
  \item Pools in which the individual $X_{ij}^*$'s are measured in addition to the poolwise $X_i^*$.
\end{itemize}

Here, we illustrate how these data types can be handled in poolwise logistic regression; the approach for linear regression and the discriminant function approach are very similar. 

Starting with replicates, suppose in the $i^{th}$ pool we observe $(Y_i, \ \bm{X_i}^*, \ \bm{C_i})$, where $\bm{X_i}^* = (X_{i1}, ..., X_{i_{k_i}})^T$. 

%Each of these three poolwise methods can be adapted to accommodate study designs that include replicate $X_i*$ measurements, gold standard $X_i*$ measurements, and pools in which the individual $X_{ij}$'s (possibly including measurement error) are measured in addition to the poolwise $X_{ij}^*$'s.




\clearpage

\section{Results}

\subsection*{Linear regression simulations}

We start by comparing efficiency of a pooling study design vs. a traditional design, for a simple linear regression scenario with no measurement error or processing error. Simulation conditions are listed in Table \ref{datagen_linear1}. In this scenario, $X$ and $C$ are moderately correlated (-0.52), $X$ and $C$ explain approximately 34\% of the variability in $Y$, and the crude $X-Y$ association is biased towards the null.

\vspace{0.2in}
\begin{table}[h]
\caption{Data generating process for linear regression simulations.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Models}\\
\midrule

$ C_{ij} \sim N(0, \ 1)$ \\
$ X_{ij} = 0.25 - 0.75 C_{ij} + \epsilon_{ij}^x, \ \epsilon_{ij} \stackrel{\text{iid}}{\sim} N(0, \ 1.5)$ \\
$ Y_{ij} = 0.25 + 0.75 X_{ij} + 1.25 C_{ij} + \epsilon_{ij}, \ \epsilon_{ij} \stackrel{\text{iid}}{\sim} N(0, \ 2.5)$

\bottomrule
\end{tabular}
\label{datagen_linear1}
\end{table}
\vspace{0.2in}

We compare the validity and efficiency of $\beta_X$ estimates for an individual design with $n_A$ total assays vs. a pooling design with the same number of assays, but twice the number of participants. To illustrate, for $n_A = 300$, the individual design was 300 single measurements, while the pooling design was 100 single measurements, 100 pools of size 2, and 100 pools of size 3, which adds to 600 total participants. We generated the individual values needed for the pooling design first, and then randomly selected a subet of size $n_A$ for the individual design. 

Both approaches were valid (not tabulated). Unfortunately, the pooling design was no more efficient than the traditional design (Figure \ref{figure_plinear1}). This is problematic because the main advantage of pooling is that it can often reduce costs or improve efficiency. Linear regression does not appear to be such a scenario. In fact, one would expect a relative loss of efficiency, as it costs money to recruit more participants for the same number of assays. 

\begin{figure}[h]
\centering
\includegraphics[height = 3.25in]{figure_plinear1.pdf}
\caption{Efficiency vs. number of assays for individual vs. pooling design in linear regression.}
\label{figure_plinear1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[height = 3.25in]{figure_plinear2.pdf}
\caption{Efficiency vs. number of assays for individual vs. pooling design in linear regression, where sorted exposure values were used to form pools.}
\label{figure_plinear2}
\end{figure}

<<eval = F, echo = F, results = "asis", fig.height = 4, fig.width = 4>>=

# Create figure showing SD vs. number of assays
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
load("plinear1_sds.rda")

setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_plinear1.pdf", height = 4.5, width = 5.5)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(1, 1))
plot(plinear1.sds[, "n_A"], plinear1.sds[, 2], lwd = 1.5, cex = 0.6, ylim = c(0, 0.12), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = "Efficiency vs. Number of Assays", xlab = expression(n[A]), ylab = "SD (1,000 trials)")
axis(side = 1, at = plinear1.sds[, "n_A"])
axis(side = 2, at = seq(0, 0.12, 0.04))
points(plinear1.sds[, "n_A"], plinear1.sds[, 2], type = "b", pch = 16, cex = 0.6, col = "blue")
points(plinear1.sds[, "n_A"], plinear1.sds[, 3], type = "b", pch = 16, cex = 0.6, col = "red")
legend("topright", horiz = F, legend = c("Individual design", "Pooling design"), pch = 16, cex = 1, col = c("blue", "red"))
dev.off()


# Also for outcome pooling
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
load("plinear2_sds.rda")

setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_plinear2.pdf", height = 4.5, width = 5.5)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(1, 1))
plot(plinear2.sds[, "n_A"], plinear2.sds[, 2], lwd = 1.5, cex = 0.6, ylim = c(0, 0.12), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = "Efficiency vs. Number of Assays", xlab = expression(n[A]), ylab = "SD (1,000 trials)")
axis(side = 1, at = plinear2.sds[, "n_A"])
axis(side = 2, at = seq(0, 0.12, 0.04))
points(plinear2.sds[, "n_A"], plinear2.sds[, 2], type = "b", pch = 16, cex = 0.6, col = "blue")
points(plinear2.sds[, "n_A"], plinear2.sds[, 3], type = "b", pch = 16, cex = 0.6, col = "red")
legend("topright", horiz = F, legend = c("Individual design", "Pooling design"), pch = 16, cex = 1, col = c("blue", "red"))
dev.off()

setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
@

We note that forming pools based on sorted $C_{ij}$ values did not affect efficiency, and sorting on $Y_{ij}$ resulted in invalid $\beta_X$ estimates.

Before moving to measurement error and processing error corrections, we wish to highlight the fact that pooling can drastically increase efficiency in linear regression when the expensive assay is for the outcome variable rather than a predictor. Figure \ref{figure_plinear2} illustrates efficiency gains of the pooling design had we been able to sort on the individual $X_{ij}$ values to form pools. This would be possible if $X$ was easy to measure and $Y$ was expensive.

Regardless, we move to the problem of estimating $\beta_X$ when the exposure is subject to various types of error. Table \ref{table_plinear3} summarizes the performance of various estimators as a function of the measurement error variance $\sigma_m^2$ and the processing error variance $\sigma_p^2$. The estimators are as follows: $\hat{\beta}_{X,IC}$ = Estimate from WLS using complete individual data on 1,200 subjects; $\hat{\beta}_{X,IN}$ = Naive estimate in individual data, not accounting for measurement error; $\hat{\beta}_{X,PN}$ = Pooled estimate assuming neither type of error; $\hat{\beta}_{X,PM}$ = Pooled estimate assuming measurement error only; $\hat{\beta}_{X,PB}$ = Pooled estimate assuming both types of error are present.

\vspace{0.2in}
<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
load("plinear3_table.rda")
#colnames(logistic.table) <- c("$OR_{ZY}$", "Complete", "Naive", "2-model ML", "2-model ML (approx.)", "2-model ML (pseudo.)", "RC (algebraic)", "PSC (no D)", "PSC (D in EP)", "PSC (D in both)")

plinear3.xtable <- xtable(plinear3.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in linear regression simulations (1,000 trials). The true value is 0.75.", label = "table_plinear3")
print(plinear3.xtable, scalebox='0.8')
@
\vspace{0.2in}

As in the previous simulation, when there was neither measurement error nor processing error, the poolwise analysis produced valid estimates with similar efficiency as the traditional design. With measurement error only, the proper pooling analysis was valid and had reasonable variability, while the naive analysis of individual data was biased. With processing error only, the traditional design was somewhat more efficient, as one would expect. With both error types, the proper pooling analysis exhibited some upward mean bias but was approximately median unbiased (0.738). Individual trials with very large sample sizes confirmed that all parameters and error components could be distinguished with the poolwise design.

When uncertain about which types of errors might be present in a poolwise dataset, accounting for both might seem to be a safe solution. However, there is a severe loss of efficiency when both are modeled but only one or neither is at play. Additionally, the $\hat{\beta}_{X,PB}$ estimator was severely right-skewed in all scenarios (Figure \ref{figure_plinear3_hist}). The appropriate measurement error only or processing error only poolwise estimators were normally distributed (not shown).

\begin{figure}[h]
\centering
\includegraphics[height = 6in]{figure_plinear3_hist.pdf}
\caption{Distribution of poolwise ML estimator that accounts for both measurement error and processing error.}
\label{figure_plinear3_hist}
\end{figure}

<<eval = F, echo = F, results = "asis", fig.height = 4, fig.width = 4>>=
# Also for outcome pooling
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")

load("plinear3.rda")
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_plinear3_hist.pdf", height = 6, width = 5.5)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(2, 2))
hist(plinear3[[6]][[1]][, 2], main = "Neither Error Type", xlab = expression(hat(beta)[X.PB]))
hist(plinear3[[6]][[2]][, 2], main = "Measurement Error Only", xlab = expression(hat(beta)[X.PB]))
hist(plinear3[[6]][[3]][, 2], main = "Processing Error Only", xlab = expression(hat(beta)[X.PB]))
hist(plinear3[[6]][[4]][, 2], main = "Both Errors Types", xlab = expression(hat(beta)[X.PB]))
dev.off()

load("plinear4.rda")
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_plinear4_hist.pdf", height = 6, width = 5.5)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(2, 2))
hist(plinear4[[7]][[1]][, 2], main = "Neither Error Type", xlab = expression(hat(beta)[X.PBR]))
hist(plinear4[[7]][[2]][, 2], main = "Measurement Error Only", xlab = expression(hat(beta)[X.PBR]))
hist(plinear4[[7]][[3]][, 2], main = "Processing Error Only", xlab = expression(hat(beta)[X.PBR]))
hist(plinear4[[7]][[4]][, 2], main = "Both Errors Types", xlab = expression(hat(beta)[X.PBR]))
dev.off()

setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
@

While the pooling design can identify both error components, we suspected that a relatively small number of replicates would greatly improve stability, as repeat measurements directly inform the measurement error variance $\sigma_m^2$. This was indeed the case. Table \ref{table_plinear4} shows the performance of the various estimators when 50 $X_{ij}$ values were measured twice, subject to independent measurement errors, and another 50 were discarded to stay at 1,200 total assays. The estimators are the same as in $\ref{table_plinear3}$, but with an extra R on subscripts where replication data were incorporated, and $\hat{\beta}_{X,IR}$ is the individual data with replicates. Figure \ref{figure_plinear3_hist} shows that the ML estimator accounting for both error types behaved much better with the 50 individual replicates.

<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
load("plinear4_table.rda")
#colnames(logistic.table) <- c("$OR_{ZY}$", "Complete", "Naive", "2-model ML", "2-model ML (approx.)", "2-model ML (pseudo.)", "RC (algebraic)", "PSC (no D)", "PSC (D in EP)", "PSC (D in both)")
plinear4.xtable <- xtable(plinear4.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in linear regression simulations (500 trials). The true value is 0.75. Estimators that end in R subscripts incorporate replicate measurements.", label = "table_plinear4")
print(plinear4.xtable, scalebox='0.8')
@

\begin{figure}[h]
\centering
\includegraphics[height = 6in]{figure_plinear4_hist.pdf}
\caption{Distribution of poolwise ML estimator that accounts for both measurement error and processing error, when 50 individual replicates are included.}
\label{figure_plinear4_hist}
\end{figure}

\clearpage

\subsection*{Logistic regression simulations}

We follow the same general process for logistic regression simulations. Data generation is summarized in Table \ref{datagen_logistic1}. $X$ and $C$ have the same distribution as before, and the crude $X-Y$ association is again biased towards the null.

\vspace{0.2in}
\begin{table}[h]
\caption{Data generating process for logistic regression simulations.}
\centering
\begin{tabular}{ll}
\toprule
\textbf{Models}\\
\midrule

$ C_{ij} \sim N(0, \ 1)$ \\
$ X_{ij} = 0.25 - 0.75 C_{ij} + \epsilon_{ij}^x, \ \epsilon_{ij} \stackrel{\text{iid}}{\sim} N(0, \ 1.5)$ \\
$ \text{logit}[P(Y_{ij}=1)] = -1 + \text{log(3)} X_{ij} + \text{log(2)} C_{ij}$

\bottomrule
\end{tabular}
\label{datagen_logistic1}
\end{table}
\vspace{0.2in}

To illustrate the procedure, suppose the total number of assays was 300. We first generated 10,000 individual observations according to Table \ref{datagen_logistic1}. Then, we randomly sampled 300 cases and controls. We used one-half of that data, 150 cases and 150 controls, for the individual estimators. For the poolwise estimators, we randomly formed 50 pools of size 1, 2, and 3 within cases and within controls. 

In contrast to the linear regression scenario, pooling on like subjects in logistic regression with no measurement or processing error can substantially improve efficiency (Figure \ref{figure_plogistic1}). 

\begin{figure}[h]
\centering
\includegraphics[height = 3.5in]{figure_plogistic1.pdf}
\caption{Efficiency vs. number of assays for individual vs. pooling design in a case-control logistic regression scenario.}
\label{figure_plogistic1}
\end{figure}

<<eval = F, echo = F, results = "asis", fig.height = 4, fig.width = 4>>=

# Create figure showing SD vs. number of assays
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
load("plogistic1_sds.rda")

setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/document")
pdf(file = "figure_plogistic1.pdf", height = 4.5, width = 5.5)
par(mar=c(4.1,5.1,3.1,2.1))
par(mfrow = c(1, 1))
plot(plogistic1.sds[, "n_A"], plogistic1.sds[, 2], lwd = 1.5, cex = 0.6, ylim = c(0, 0.16), yaxt = "n",
     type = "n", pch = 16, col = "black", xaxt = "n",
     main = "Efficiency vs. Number of Assays", xlab = expression(n[A]), ylab = "SD (1,000 trials)")
axis(side = 1, at = plogistic1.sds[, "n_A"])
axis(side = 2, at = seq(0, 0.25, 0.05))
points(plogistic1.sds[, "n_A"], plogistic1.sds[, 2], type = "b", pch = 16, cex = 0.6, col = "blue")
points(plogistic1.sds[, "n_A"], plogistic1.sds[, 3], type = "b", pch = 16, cex = 0.6, col = "red")
legend("topright", horiz = F, legend = c("Individual design", "Pooling design"), pch = 16, cex = 1, col = c("blue", "red"))
dev.off()
@


Table \ref{table_plogistic2} summarizes the performance of various individual and pooled estimators. The ML estimator appeared to exhibit some mean bias, although it was much less variable than in the linear regression setting. Further simulations are in progress.


%Regardless, we move to the problem of estimating $\beta_X$ when the exposure is subject to various types of error. Table \ref{table_plinear3} summarizes the performance of various estimators as a function of the measurement error variance $\sigma_m^2$ and the processing error variance $\sigma_p^2$. The estimators are as follows: $\hat{\beta}_{X,IC}$ = Estimate from WLS using complete individual data on 1,200 subjects; $\hat{\beta}_{X,IN}$ = Naive estimate in individual data, not accounting for measurement error; $\hat{\beta}_{X,PN}$ = Pooled estimate assuming neither type of error; $\hat{\beta}_{X,PM}$ = Pooled estimate assuming measurement error only; $\hat{\beta}_{X,PB}$ = Pooled estimate assuming both types of error are present.

\vspace{0.2in}
<<eval = T, echo = F, results = "asis">>=
setwd("C:/Users/Dane/Google Drive/Documents/Research/Dissertation/cluster/psims_pooling")
load("plogistic2_table.rda")
plogistic2.xtable <- xtable(plogistic2.table, caption = "Mean (SD) of various estimators for $\\beta_X$ in logistic regression simulations (1,000 trials). The true value is log(3) = 1.099.", label = "table_plogistic2")
print(plogistic2.xtable, scalebox='0.8')
@




% \clearpage
% 
% \section{Software}
% 
% We are currently developing an R package with functions for performing various poolwise analyses. Functions include the following:
% 
% \subsubsection*{poolwise.linear}
% 
% Linear regression analysis of poolwise data assuming no measurement error or processing error. It essentially passes the user-input poolwise data to the \textit{lm} function to obtain WLS estimates for parameters in the individual-level linear regression model \ref{linear_individual_model}.
% 
% \subsubsection*{poolwise.linear.xerrors}
% 
% ML analysis for poolwise linear regression where a predictor is subject to measurement error, processing error, or both. The user inputs poolwise data $(Y_i, \ g_i, \ X_i^*, \ \bm{C}_i^*)$, and the function returns estimates parameters in the individual-level linear regression model \ref{linear_individual_model}, assuming model \ref{xc_individual} for $X_{ij}|\bm{C}_{ij}$. When $\bm{C}$ is empty, the function assumes marginal normality for $X_{ij}$. The function returns a vector of estimated parameters and a variance-covariance matrix. The function also accommodates replication data, i.e. $(Y_i, \ g_i, \ X_{i1}^*, ... , \ X_{i m_i}^*, \ \bm{C}_i)$.
% 
% \subsubsection*{poolwise.linear.yerrors}
% 
% ML analysis for poolwise linear regression where the outcome is subject to measurement error, processing error, or both. With errors in the outcome variable, one can obtain valid estimates of regression parameters for virtually any pooling design (or individual measurements), even if the error components cannot be distinguished. Thus one could ignore the errors and call \textit{poolwise.linear} instead. When there are multiple pool sizes, this function should be more efficient.
% 
% \subsubsection*{poolwise.logistic}
% 
% Estimation of regression parameters for the \citet{weinberg1999using} poolwise logistic regression model, assuming no mesaurement error or processing error. 
% 
% \subsubsection*{poolwise.logistic.xerrors}
% 
% ML analysis for the \citet{weinberg1999using} poolwise logistic regression model where a predictor is subject to measurement error, processing error, or both.
% 
% \subsubsection*{poolpower.onesamplet}
% 
% For visualizing the relationship between statistical power and total costs for individual vs. pooling study designs in the one-sample t-test scenario. The user specifies the usual parameters for calculating power (true difference from $\mu_0$, standard deviation) and the cost to perform each assay and to recruit each participant. The purpose is to help investigators determine whether a pooling design can reduce costs and/or improve power.
% 
% \subsubsection*{poolpower.twosamplet}
% 
% Power analysis for individual vs. pooled designs in two-sample t-test scenario.
% 
% \subsubsection*{poolpower.anova}
% 
% Power analysis for individual vs. pooled designs in analysis of variance scenario.
% 
% 
% 
% \clearpage

\clearpage

\section{Discussion}

Pooling study designs remain rare in epidemiology, despite numerous advantages (reduce costs, preserve specimen volume, potentially increase power). Perhaps one reason is that analyzing poolwise data requires nonstandard methods, which become even more challenging when practical issues like measurement error arise. Here, we build on previous work by \citet{weinberg1999using, schisterman2010hybrid, lyles2015discriminant} and we present maximum likelihood methods for handling measurement error, processing error, or both by utilizing the pooling study design itself.

A pooling study design that facilitates a measurement error correction in the analysis phase, without validation data, seems very appealing. One might think to account for both types of error in all poolwise analysis to protect against either type of error in case it is present. Unfortunately, we found that modeling both types of error when only one or neither is present resulted in very inefficient estimation of exposure effects, at least in the linear regression setting. In general, the ML estimator seemed to perform well when one or the other error was present, but was badly skewed when both were present, even for fairly large sample sizes.

On a more positive note, we found that adding a small number of replicate individual measurements greatly stabilized the ML estimator that accounted for both error types. We plan to explore this further, but it seems that this design may allow for efficient estimation of the exposure effect, while little efficiency lost by simplying always assuming that both types of errors could be present.

We mainly considered a pooling design that included an equal number of pools of size 1, 2, and 3. To identify all variance components, at least two different pool sizes are needed in addition to individual measurements \citet{lyles2015discriminant}. However, we note that while individual measurements are needed to distinguish $\sigma_m^2$ from $\sigma_p^2$, a design with three different pool sizes not including $g = 1$ can estimate their sum and, more importantly, produce valid estimates of $\beta_X$.

Another advantage of replicate individual measurements is that it permits direct estimation of all regression parameters and variance components, except $\sigma_p^2$, even without incorporating other non-replicate individual or pooled measurements. So any pooling design that incorporates some individual replicates will be able to identify all parameters of interest, including each variance component, regardless of the pool sizes. In a sense, this makes the prospect of adding replicates less burdensome. Rather than a pooling design with pool sizes of 1, 2, and 3, one could use a single pool size, say $g = 2$, plus a small number of individual replicates. In fact doing so would very likely produce more precise estimate of $\beta_X$.

We are currently investigating whether the presence of measurement error affects the relative efficiency of a pooling design vs. a traditional design. The appropriate comparison would seem to be a traditional design with replicates to facilitate a mesaurement error correction vs. a pooling design that also includes some replicates. Simulations seem to suggest that measurement error does not affect the relative efficiency of a pooling vs. individual design, but substantial processing error could hurt the pooling design.

%In the absence of measurement and processing errors, pooling had an efficiency advantage for logistic regression with homogeneous pools, but not for linear regression (in a slightly different scenario where $X$ is prone to errors, but $Y$ requires an expensive error-free assay, forming pools on $X$ would be valid and improve efficiency). For the traditional design, correcting for measurement error requires replicates. 

%pooling like subjects resulted in an efficiency gain for logistic regression for \citet{weinberg1999using} case-control logistic regression setup, but 

%In linear regression, there was no efficiency advantage to start with, making it unlikely that when measurement 

%In linear regression, we are currently investigating whether a pooling design can give more precise $\beta_X$ estimates than a traditional design when there is measurement error. It may seem unlikely, considering that there was no efficiency advantage to start with, and pooled measurements are prone to the same measurement error as individual measurements, plus processing error. But if measurement errors for pooled observations are the same regardless of the pool size $g$, the total variance of a pooled observation with $g > 1$ would be $g V(X) + \sigma_m^2 + \sigma_p^2$, while the total variance of $g$ individual observations would be $g V(X) + g \sigma_m^2$. The total variance is smaller in the pooled measurement as long as $g \sigma_m^2 > \sigma_m^2 + \sigma_p^2$. 

%.  measurement error just as individual measurements. pooling designs are no more efficient than individual designs for linear regression without measurement error. 



\mychapter{2}{Chapter 3: Future direction of pooling research}

In Chapter 2, we presented likelihood methods for fitting linear and logistic regression models using poolwise data, with a pooled exposure subject to measurement and processing errors. We are considering several extensions of these methods. 

One limitation of our approach was that it assumed a normal linear model for the error-prone $X$ given covariates $\bm{C}$, or marginal normality of $X$ in the absence of covariates. When $X$ is a right-skewed biomarker, a linear model for log-transformed $X$ values vs. $\bm{C}$ might be more appropriate. \citet{mitchell2015semiparametric, mitchell2014regression} considered a similar scenario, where the outcome variable rather than an exposure was right-skewed. We plan to explore incorporating measurement and processing error into the parametric and semi-parametric methods proposed in those papers. 


% 
% 
% \section{Discussion}
% 
% \begin{itemize}
%   \item A practical problem that may be holding pooling back is measurement/processing error. Try to fix that.
%   \item More power... pooling should be more common!
%   \item How to get more power.
%   \item Software.
%   \item Binary outcome where logistic regression works - gain a lot by pooling.
%   \item Linear regression where X is subject to pooling - only gain anything if there is another covariate. BUT maybe you can flip Y and X.
%   \item Compare to individual setting with measurement error... Even in the simplest additive normal setting, have to design study very carefully and use special methods for replicate/validation data. With pooling, just need different pool sizes. Then call linear regression function and say you want to correct for measurement error and/or processing error. Can see if it really makes any difference. 
%   \item What about pooling design as solution to measurement error when no gold standard is available? But replicates on individuals is probably better.
%   \item Replicates are feasible and probably more useful to estimate measurement error variance. Think it's best to do replicates in individual measurements, but if you DID do it in pools, would separate processing errors occur each time, or the same one?
%   \item 
% \end{itemize}






%%%%

% \section{Things to consider}
% 
% Remove ``Complete'' column from results tables. Added a figure to Scenario 3 that compares vs. Complete column. 
% 
% 1. Weller 2007 approach equivalent to minimizing least squares? Have overdetermined system, so multiple estimates of betax. See which is better if they are different. 
% 
% 2. What about strictly applying RC1 to case of multiple surrogates for single exposure.
% 
% 3. Optimizing RC estimator when validation data are internal. When surrogacy holds, Weller 2007 estimator works...
% 
% 4. Best practices for PSC... what data to use for what. 
% 
% 5. Leaving error-prone PS out of TDM... assumption that it does not inform Y conditional on X and gold standard PS. Maybe weaker assumption, this OR it is not correlated with X? If related to Y but not X then it will not affect the estimated exposure effect. Some way to modify the method to ensure lack of correlation with X? Does that even make sense?
% 
% 6. PSC with continuous rather than binary exposure.
% 
% 7. Does RC work in scenario 1 when the MEM is nonlinear?
% 
% 8. For ``not enough money to measure on everyone'' argument for pooling, why not do random sample... Need to include this approach.
% 
% 9. Add scenario where ML works without a D. Maybe do it in logistic regression setting. Think you need binary Z.
% 
% 10. Somehow utilize fact that two-sample t-test is equivalent to linear regression with binary group variable...
% 
% 11. Include Betas in ML densities?
% 
% 12. More consistent tense (we vs. one vs. passive).
% 
% 13. What about PSC from the viewpoint of ML... plugging in expressions for P(A|Xgs) and P(A|Xep), etc. Gotta be something there.
% 
% 14. Regression calibration for binary exposure/confounder... see Cole paper. Think they used linear regression!
%
% 15. Effect of centering or standardizing pooled covariate data.
%
% 16. Need good real data to see how measurement error and processing error compare for various assays. Hopefully processing error is small... otherwise pooling design could lose any efficiency it had over individual design.
%
% 17. Intuitively, more measurement error favors bigger pools, more processing error favors individual measurements.
% 
% \section{References to add}
% 
% UC: 
% 
% 1. Liang \& Liu paper/textbook chapter
% 
% 2. Liu \& Liang Stat Med paper (binary exposure)
% 
% 3. Lyles \& Kupper 1997
% 
% Pooling:
% 
% 1. Enrique, Emily, Bob




%' 
%' 
%' %Another distinction is that using external validation data requires a ``transportability' assumption--that the X-W relationship is the same in the external validation population as it is in the main study population \citepcarroll2006measurement. For internal validation data, assuming that the subset of participants with $X$ measured is random, no such assumption is necessary. The transportability is usually considered plausible when demographics and other characteristics of the main study participants and external validation study participants are similar.\\
%' 
%' 
%' 
%' % \subsection*{Two-Model Paradigm}
%' % 
%' % A two-model paradign has proven useful for implementing maximum likelihood and regression calibration in measurement error problems \citep{spiegelman1991cost, lyles2013approximate, rosner1989correction}. Following the notation of Lyles \& Kupper, there are two assumed models. The true disease model (TDM) relates the outcome Y to an unmeasured confounder, Z, and a covariate vector $\bm{C} = (C_1, ... , C_p)^T$. We may be primarily interested in the adjusted association between one of the C's and Y, or we may have equal interest in each of the C's. The measurement error model (MEM) relates Z to the same covariate vector as in the TDM, $\bm{C}$, and optionally a set of additional predictors, $\bm{D}$. For example, consider the linear TDM and linear MEM:
%' % 
%' % \begin{equation}
%' % \begin{split}
%' % \text{TDM} \quad & Y = \beta_0 + \beta_Z Z + \bm{\beta_C}^T \bm{C} + \epsilon, \qquad \epsilon \stackrel{iid}{\sim} (0, \sigma_\epsilon^2) \\
%' % \text{MEM} \quad & Z = \alpha_0 + \bm{\alpha_C}^T \bm{C} + \bm{\alpha_d}^T \bm{D} + \delta, \quad \ \delta \stackrel{iid}{\sim} (0, \sigma_\delta^2)
%' % \end{split}
%' % \end{equation}
%' % \vspace{4pt}
%' % 
%' % Typically there is no real interest in $\bm{\alpha} = (\alpha_0, \bm{\alpha_C}^T)^T$, and there would be no need to even consider the MEM if we observed $(Y, Z, \bm{C}^T)$ for all study participants. But specifying the MEM allows us to estimate $\bm{\beta}$ using all available data, which would be $(Y, \bm{C}^T)^T$ for main study participants, $(Y, Z, \bm{C}^T, \bm{D}^T$ for internal validation study participants, and $(Z, \bm{C}^T, \bm{D}^T)^T$ for external validation study participants. Usually there would be main study and internal validation study participants, or main study and external validation study participants, but there could also be all three types.\\
%' % 
%' % In contrast to the traditional setup where a single variable serves as an error-prone version of $Z$, in this scenario we view $(\bm{C}^T, \bm{D}^T)^T$ as together explaining some amount of variability in $Z$. This setup is more general than Berkson error, which is a very specific special case where $\alpha_0 = 0$, $\bm{\alpha_C} = 0$ and all but one element in $\bm{\alpha_d}$ is 0, while the non-zero one is 1. In that scenario, the single $D$ is an unbiased version of $Z$, and we can simply fit the TDM with $D$ in place of $Z$ and obtain valid estimates for $\bm{\beta}$. But variance estimates for $\hat{\bm{\beta}}$ would be inflated compared to the scenario where $Z$ is measured without error.
%' % 
%' % 
%' % 
%' % \subsection*{Maximum Likelihood}
%' % 
%' % If one is willing to specify the distribution of $Y|Z, \bm{C}$ and $Z|\bm{C}, \bm{D}$, maximum likelihood can be used to estimate the adjusted associations between Y and the TDM predictors $\bm{C}$.\\
%' % 
%' % Suppose the density for $Y$ given $Z$ and $\bm{C}$ is $f_1(y; \bm{\beta}, \bm{\psi_1})$, where $\bm{\beta}$ represents TDM regression parameters and $\bm{\psi_1}$ represents additional nuisance parameters, and the density for $Z$ given $\bm{C}$ and $\bm{D}$ is $f_2(z; \bm{\alpha}, \bm{\psi_2})$, where $\bm{\alpha}$ are MEM regression parameters and $\bm{\psi_2}$ are nuisance parameters. We wish to estimate $\bm{\theta} = (\bm{\beta}^T, \bm{\psi_1}^T, \bm{\alpha}^T, \bm{\psi_2}^T)^T$.\\
%' % 
%' % \subsubsection*{Main Study Participants}
%' % 
%' % The likelihood contribution for main study participants with $(Y, \bm{C}^T, \bm{D}^T)^T$ observed is:
%' % 
%' % \begin{equation}
%' % L_m(\bm{\theta}) = f(Y, \bm{C}, \bm{D}) = \int_z f(Y, Z, \bm{C}, \bm{D}) dz = \int_z f(Y | Z, \bm{C}, \bm{D}) f(Z | \bm{C}, \bm{D}) f(\bm{D}, \bm{C}) dz
%' % \end{equation}
%' % 
%' % By assumption, $f(Y | Z, \bm{C}, \bm{D}) = f(Y | Z, \bm{C}) = f_1(y; \bm{\beta}, \bm{\psi_1})$. We also have $f(Z | \bm{C}, \bm{D}) = f_2(z; \bm{\alpha}, \bm{\psi_2})$. We can pull the joint density $f(\bm{C}, \bm{D})$ outside of the integral since it does not depend on z, and then drop this term entirely under the assumption that the density is not indexed by any parameters of interest. Thus $L_m$ simplifies to:
%' % 
%' % \begin{equation}
%' % L_M(\bm{\theta}) = \int_z f_1(y; \bm{\beta}, \bm{\psi_1}) f_2(z; \bm{\alpha}, \bm{\psi_2}) dz
%' % \end{equation}
%' % 
%' % When both $Y|Z, \bm{C}$ and $Z | \bm{C}, \bm{D}$ are normal, a ``completing the square'' approach can be used to obtain a closed-form solution for the integral. In other scenarios, the integral can be solved numerically.\\
%' % 
%' % \subsubsection*{Internal Validation Study Participants}
%' 
%' % The likelihood contribution for internal validation study participants with $(Y, Z, \bm{C}^T, \bm{D}^T)^T$ observed is:
%' % 
%' % \begin{equation}
%' % L_I(\bm{\theta}) = f(Y, Z, \bm{C}, \bm{D}) = f(Y | Z, \bm{C}, \bm{D}) f(Z | \bm{C}, \bm{D}) f(\bm{C}, \bm{D})
%' % \end{equation}
%' % 
%' % Again, we take advantage of the fact that $f(Y | Z, \bm{C}, \bm{D}) = f(Y | Z, \bm{C})$, and we drop the last term since it does not depend on $\bm{\theta}$, to arrive at:
%' % 
%' % \begin{equation}
%' % L_I(\bm{\theta}) = f_1(y; \bm{\beta}, \bm{\psi_1}) f_2(z; \bm{\alpha}, \bm{\psi_2})
%' % \end{equation}



\bibliographystyle{jss}
\bibliography{bibtexfile}


\end{document}